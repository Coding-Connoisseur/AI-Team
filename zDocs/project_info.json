{
    "system_info": {
        "os": "Linux",
        "os_version": "#26~22.04.1-Ubuntu SMP Thu Jul 11 22:33:04 UTC 2024",
        "python_version": "3.12.1",
        "installed_libraries": [
            "setuptools==75.1.0",
            "certifi==2024.8.30",
            "psutil==6.0.0",
            "python-json-logger==2.0.7",
            "urllib3==2.2.3",
            "Send2Trash==1.8.3",
            "traitlets==5.14.3",
            "defusedxml==0.7.1",
            "matplotlib-inline==0.1.7",
            "bleach==6.1.0",
            "jupyterlab_git==0.50.1",
            "soupsieve==2.6",
            "webcolors==24.8.0",
            "pandas==2.2.3",
            "platformdirs==4.3.6",
            "GitPython==3.1.43",
            "jupyter-server-mathjax==0.2.6",
            "jsonschema==4.23.0",
            "seaborn==0.13.2",
            "PyYAML==6.0.2",
            "rfc3339-validator==0.1.4",
            "async-lru==2.0.4",
            "prompt_toolkit==3.0.48",
            "httpx==0.27.2",
            "types-python-dateutil==2.9.0.20240906",
            "torch==2.4.1+cpu",
            "six==1.16.0",
            "jupyterlab_server==2.27.3",
            "notebook_shim==0.2.4",
            "colorama==0.4.6",
            "jupyter_server==2.14.2",
            "pillow==10.4.0",
            "MarkupSafe==2.1.5",
            "ptyprocess==0.7.0",
            "filelock==3.13.1",
            "terminado==0.18.1",
            "numpy==2.1.1",
            "fsspec==2024.2.0",
            "pexpect==4.9.0",
            "Jinja2==3.1.4",
            "comm==0.2.2",
            "argon2-cffi-bindings==21.2.0",
            "jupyter-events==0.10.0",
            "scikit-learn==1.5.2",
            "requests==2.32.3",
            "debugpy==1.8.6",
            "uri-template==1.3.0",
            "gitdb==4.0.11",
            "stack-data==0.6.3",
            "idna==3.10",
            "prometheus_client==0.21.0",
            "nbdime==4.0.2",
            "jupyter-lsp==2.2.5",
            "tinycss2==1.3.0",
            "pyparsing==3.1.4",
            "pyzmq==26.2.0",
            "fonttools==4.54.1",
            "python-dateutil==2.9.0.post0",
            "wcwidth==0.2.13",
            "ipython==8.27.0",
            "argon2-cffi==23.1.0",
            "babel==2.16.0",
            "jupyter_server_terminals==0.5.3",
            "nbformat==5.10.4",
            "contourpy==1.3.0",
            "sniffio==1.3.1",
            "scipy==1.14.1",
            "smmap==5.0.1",
            "networkx==3.2.1",
            "isoduration==20.11.0",
            "pure_eval==0.2.3",
            "matplotlib==3.9.2",
            "tornado==6.4.1",
            "charset-normalizer==3.3.2",
            "mpmath==1.3.0",
            "jedi==0.19.1",
            "tzdata==2024.2",
            "decorator==5.1.1",
            "cycler==0.12.1",
            "arrow==1.3.0",
            "pycparser==2.22",
            "nbconvert==7.16.4",
            "parso==0.8.4",
            "webencodings==0.5.1",
            "attrs==24.2.0",
            "jupyterlab_pygments==0.3.0",
            "Pygments==2.18.0",
            "overrides==7.7.0",
            "json5==0.9.25",
            "rfc3986-validator==0.1.1",
            "anyio==4.6.0",
            "jsonschema-specifications==2023.12.1",
            "fqdn==1.5.1",
            "asttokens==2.4.1",
            "websocket-client==1.8.0",
            "jupyter_client==8.6.3",
            "fastjsonschema==2.20.0",
            "mistune==3.0.2",
            "jupyter_core==5.7.2",
            "nbclient==0.10.0",
            "plotly==5.24.1",
            "beautifulsoup4==4.12.3",
            "cffi==1.17.1",
            "executing==2.1.0",
            "threadpoolctl==3.5.0",
            "h11==0.14.0",
            "sympy==1.12",
            "jupyterlab==4.2.5",
            "jsonpointer==3.0.0",
            "joblib==1.4.2",
            "referencing==0.35.1",
            "pytz==2024.2",
            "kiwisolver==1.4.7",
            "pandocfilters==1.5.1",
            "tenacity==9.0.0",
            "packaging==24.1",
            "httpcore==1.0.5",
            "nest-asyncio==1.6.0",
            "rpds-py==0.20.0",
            "ipykernel==6.29.5",
            "pip==24.2",
            "pydantic_core==2.23.4",
            "typing_extensions==4.12.2",
            "jiter==0.6.1",
            "graphviz==0.20.3",
            "openai==1.51.2",
            "pydantic==2.9.2",
            "tqdm==4.66.5",
            "python-dotenv==1.0.1",
            "annotated-types==0.7.0",
            "distro==1.9.0"
        ]
    },
    "file_structure": {
        ".": [
            "create_github_issues.py",
            "test_task_priority_queue.py",
            "system.py",
            ".gitignore",
            "knowledge_base.py",
            "test_load_balancer.py",
            "requirements.txt",
            "agent_health_monitor.py",
            "agents.py",
            "stories.json",
            "test_health_check_manager.py",
            "Dockerfile",
            "test_knowledge_base.py",
            "test_main.py",
            ".env",
            "task_priority_queue.py",
            "load_balancer.py",
            "health_check_manager.py",
            "test_agent_health_monitor.py",
            "test_task_monitor.py",
            "test_dynamic_thread_pool.py",
            "test_system.py",
            "task_monitor.py",
            "test_agents.py",
            "system_monitor.py",
            "dynamic_thread_pool.py",
            "main.py"
        ],
        "__pycache__": [
            "system_monitor.cpython-312.pyc",
            "agent_health_monitor.cpython-312.pyc",
            "load_balancer.cpython-312.pyc",
            "task_priority_queue.cpython-312.pyc",
            "agents.cpython-312.pyc",
            "health_check_manager.cpython-312.pyc",
            "task_monitor.cpython-312.pyc",
            "knowledge_base.cpython-312.pyc",
            "system.cpython-312.pyc",
            "dynamic_thread_pool.cpython-312.pyc"
        ],
        "zDocs": [
            "call_graph.png"
        ],
        "real_project": [],
        ".venv": [
            ".gitignore",
            "pyvenv.cfg"
        ],
        "zScripts": [
            "unpackStructure.py",
            "createOverview.py",
            "SNAPSHOT.py",
            "createCallGraph.py"
        ],
        ".pytest_cache": [
            ".gitignore",
            "CACHEDIR.TAG",
            "README.md"
        ]
    },
    "dependencies": {
        "python": [
            "psutil"
        ]
    },
    "git_history": [
        "3fafbaf - Refactor .gitignore to ignore unnecessary files and directories",
        "066efaf - Stop tracking __pycache__",
        "28807f7 - Refactor ignore patterns and system information capture",
        "fcadc9a - Refactor ignore patterns and system information capture",
        "53dc89c - Merge branch 'main' of https://github.com/Coding-Connoisseur/AI-Team",
        "a99a09d - Refactor project path input validation in TeamLeaderAI class",
        "654b049 - Add files via upload",
        "9d93c10 - delete call graph",
        "3d7b8d5 - Refactor project path input validation in TeamLeaderAI class Refactor OpenAI client initialization in agents.py and exclude unnecessary Python cache files from version control Assign tasks to agents based on availability and priorities Remove unused files createOverview.py and unpackStructure.py Implement retry logic with exponential backoff in recover_from_failure method of TeamLeaderAI class Handle circular dependencies in get_next_task method of TaskPriorityQueue class",
        "c18bce9 - Refactor project path input validation in TeamLeaderAI class Refactor OpenAI client initialization in agents.py and exclude unnecessary Python cache files from version control Add createOverview.py script to combine files in a directory into a single output file Add unpackStructure.py script to parse the Project_Overview.txt file and create the project structure Add createCallGraph.py script to generate a call graph of functions and classes in a directory"
    ],
    "files": {
        "create_github_issues.py": {
            "contents": "\nimport requests\nfrom datetime import datetime, timedelta\n\n# GitHub repository details\nrepo_owner = 'Coding-Connoisseur'\nrepo_name = 'AI-Team'\naccess_token = 'your_github_token'  # Replace with a valid GitHub token\n\n# GitHub API URL for creating issues\nissues_url = f'https://api.github.com/repos/{repo_owner}/{repo_name}/issues'\n\n# Sample user stories\nuser_stories = [\n    {\"title\": \"Agent Enhancement and Learning\", \"tasks\": [\n        {\"description\": \"Implement success rate tracking for agents\", \"due_date\": \"2024-11-01\"},\n        {\"description\": \"Enable agents to adjust behavior based on success rates\", \"due_date\": \"2024-11-07\"}\n    ]},\n    {\"title\": \"Project Setup\", \"tasks\": [\n        {\"description\": \"Allow users to specify project type\", \"due_date\": \"2024-11-03\"},\n        {\"description\": \"Dynamically create architecture based on project type\", \"due_date\": \"2024-11-10\"}\n    ]},\n    # Additional stories here...\n]\n\n# Headers for GitHub API request\nheaders = {\n    'Authorization': f'token {access_token}',\n    'Accept': 'application/vnd.github.v3+json'\n}\n\ndef create_github_issue(title, body):\n    \"\"\"Creates an issue on GitHub.\"\"\"\n    issue_data = {\n        'title': title,\n        'body': body\n    }\n    response = requests.post(issues_url, json=issue_data, headers=headers)\n    if response.status_code == 201:\n        print(f\"Issue '{title}' created successfully!\")\n    else:\n        print(f\"Failed to create issue '{title}'. Status Code: {response.status_code}, Response: {response.json()}\")\n\ndef main():\n    for story in user_stories:\n        # Create the main issue for each user story\n        title = story['title']\n        body = f\"**User Story**: {title}\\n\\n**Tasks**:\\n\"\n        for task in story['tasks']:\n            due_date = task['due_date']\n            body += f\"- {task['description']} (Due: {due_date})\\n\"\n\n        create_github_issue(title, body)\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        "test_task_priority_queue.py": {
            "contents": "# test_task_priority_queue.py\nimport unittest\nfrom task_priority_queue import TaskPriorityQueue\nclass TestTaskPriorityQueue(unittest.TestCase):\n    def setUp(self):\n        self.priority_queue = TaskPriorityQueue()\n    def test_enqueue_task(self):\n        self.priority_queue.enqueue(\"task1\", priority=1)\n        self.priority_queue.enqueue(\"task2\", priority=3)\n        self.priority_queue.enqueue(\"task3\", priority=2)\n        self.assertEqual(len(self.priority_queue.queue), 3)\n    def test_dequeue_task(self):\n        self.priority_queue.enqueue(\"task1\", priority=1)\n        self.priority_queue.enqueue(\"task2\", priority=3)\n        task = self.priority_queue.dequeue()\n        self.assertEqual(task, \"task1\")\n    def test_queue_emptiness(self):\n        self.priority_queue.enqueue(\"task1\", priority=1)\n        self.priority_queue.dequeue()\n        self.assertTrue(self.priority_queue.is_empty())\n    def test_peek_task(self):\n        self.priority_queue.enqueue(\"task1\", priority=1)\n        self.priority_queue.enqueue(\"task2\", priority=3)\n        task = self.priority_queue.peek()\n        self.assertEqual(task, \"task1\")\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        "system.py": {
            "contents": "from task_priority_queue import TaskPriorityQueue\nfrom task_monitor import TaskMonitor\nfrom agent_health_monitor import AgentHealthMonitor\nfrom system_monitor import SystemMonitor\nfrom health_check_manager import HealthCheckManager\nfrom dynamic_thread_pool import DynamicThreadPoolExecutor\nfrom load_balancer import LoadBalancer\nimport os\nimport time\n\nclass TeamLeaderAI:\n    def __init__(self, agents, knowledge_base, retry_limit=3):\n        self.agents = agents\n        self.knowledge_base = knowledge_base\n        self.task_priority_queue = TaskPriorityQueue(SystemMonitor())\n        self.task_progress = {}  # Track task states\n        self.task_completion_data = {}  # Track completed task details\n        self.retry_limit = retry_limit\n        self.task_retries = {}\n        self.thread_pool = DynamicThreadPoolExecutor(max_workers=3)\n        self.load_balancer = LoadBalancer(self.agents.values(), self.knowledge_base)\n        self.task_monitor = TaskMonitor()\n        self.agent_health_monitor = AgentHealthMonitor(self.agents.values(), self.load_balancer)\n        self.system_monitor = SystemMonitor()\n        self.health_check_manager = HealthCheckManager(self.system_monitor, self.agent_health_monitor)\n        self.project_path = None\n        self.active_tasks = {}\n        self.completed_tasks = []\n        self.project_type = None\n\n    def get_user_input(self):\n        print(\"What do you want the AI team to do? Choose from the following options:\")\n        print(\"1. Create a whole project\")\n        print(\"2. Enhance an existing project\")\n        print(\"3. Debug a project\")\n        print(\"4. Add new features and capabilities to a project\")\n        print(\"5. Test a project\")\n        choice = input(\"Enter the number of your choice: \")\n        return int(choice)\n    \n    def ask_for_project_path(self):\n        while True:\n            path = input(\"Enter the full path to your project directory: \")\n            if os.path.isdir(path):\n                return path\n            else:\n                print(\"Invalid directory path. Please enter a valid path.\")\n    \n    def receive_user_input(self):\n        choice = self.get_user_input()\n        if choice == 1:\n            print(\"You selected to create a whole project.\")\n            self.project_type = input(\"Enter the type of project (e.g., web app, API, machine learning, etc.): \")\n            self.decompose_project(f\"Create a {self.project_type}\")\n        elif choice in [2, 3, 4, 5]:\n            if not self.ask_for_project_path():\n                return\n            task_overview = {\n                2: \"Enhance project\",\n                3: \"Debug project\",\n                4: \"Add features and capabilities\",\n                5: \"Test project\"\n            }\n            print(f\"You selected to {task_overview[choice].lower()}.\")\n            self.decompose_project(task_overview[choice])\n        else:\n            print(\"Invalid choice.\")\n\n    def decompose_project(self, overview):\n        tasks = [\n            (1, \"architecture design\"),\n            (2, \"code generation\"),\n            (3, \"debugging\"),\n            (4, \"testing\"),\n            (5, \"enhancement\"),\n            (6, \"documentation\"),\n            (7, \"deployment\"),\n            (8, \"security audit\"),\n            (9, \"database setup\"),\n            (10, \"logging setup\"),\n            (11, \"version control\"),\n            (12, \"frontend generation\")\n        ]\n        for priority, task in tasks:\n            self.task_priority_queue.add_task(priority, task, \"medium\")\n        self.assign_tasks()\n\n    def assign_tasks(self):\n        while task_name := self.task_priority_queue.get_next_task():\n            dependencies = self.task_priority_queue.task_dependencies.get(task_name, set())\n            if not dependencies:\n                agent = self.load_balancer.assign_task(task_name)\n                # Pass project_type if required by the agent\n                if hasattr(agent, 'execute_task'):\n                    self.thread_pool.submit_task(\n                        agent.execute_task, task_name, self.project_type if task_name == \"architecture design\" else None\n                    )\n                self.update_task_status(task_name, 'active', agent.name)\n            else:\n                print(f\"Task '{task_name}' is waiting for dependencies: {dependencies}\")\n\n\n    def find_agent_for_task(self, task_name):\n        for agent in self.agents.values():\n            if agent.can_handle(task_name):\n                return agent\n        return None\n    \n    def execute_task(self, agent, task_name):\n        try:\n            start_time = time.time()\n            self.task_monitor.start_task(task_name)\n            outcome = agent.execute_task(task_name)\n            elapsed_time = self.task_monitor.end_task(task_name)\n            self.task_completion_data[task_name] = {\n                'status': 'completed',\n                'agent': agent.name,\n                'duration': elapsed_time,\n                'outcome': outcome\n            }\n            \n            # Mark the task as complete and resolve dependencies\n            self.task_priority_queue.mark_task_complete(task_name)\n            self.update_task_status(task_name, 'completed', agent.name)\n            print(f\"Task '{task_name}' completed by {agent.name} with outcome: {outcome}\")\n\n        except Exception as e:\n            print(f\"Error in executing task '{task_name}' by {agent.name}: {e}\")\n            self.record_failure(task_name, agent)\n            self.task_priority_queue.update_task_priority(task_name, priority=0)  # Escalate priority on failure\n\n    def handle_agent_feedback(self, task_name, result):\n        if result == \"success\":\n            self.task_progress[task_name] = \"completed\"\n        else:\n            self.recover_from_failure(task_name)\n\n    def recover_from_failure(self, task):\n        \"\"\"\n        Attempts to retry the task with exponential backoff.\n        \n        Args:\n            task (str): The task to retry.\n        \"\"\"\n        max_delay = 60  # Set a maximum delay to avoid excessively long waits\n        base_delay = 1  # Initial delay in seconds\n\n        # Calculate the delay based on the current retry count with exponential backoff\n        retries = self.task_retries.get(task, 0)\n        delay = min(base_delay * (2 ** retries), max_delay)\n\n        if retries < self.retry_limit:\n            self.task_retries[task] = retries + 1\n            print(f\"Retrying task '{task}' with a delay of {delay} seconds (Attempt {retries + 1}/{self.retry_limit}).\")\n            time.sleep(delay)  # Apply exponential backoff delay\n            agent = self.find_agent_for_task(task)\n            if agent:\n                self.execute_task(agent, task)\n        else:\n            print(f\"Task '{task}' has exceeded the retry limit after {retries} attempts.\")\n\n    def update_task_status(self, task_name, status, agent_name):\n        self.task_progress[task_name] = {'status': status, 'agent': agent_name}\n\n    def mark_task_completed(self, task_name, agent):\n        if task_name in self.active_tasks:\n            del self.active_tasks[task_name]\n        self.completed_tasks.append({\n            'task': task_name,\n            'agent': agent.name,\n            'status': 'completed'\n        })\n        self.update_dashboard()\n\n    def report_progress(self):\n        print(\"\\n--- Task Progress Dashboard ---\")\n        print(\"Queued Tasks:\")\n        print(\"  No queued tasks.\")\n        print(\"\\nActive Tasks:\")\n        for task, agent in self.active_tasks.items():\n            print(f\"  {task} (Agent: {agent.name})\")\n        print(\"\\nCompleted Tasks:\")\n        if not self.completed_tasks:\n            print(\"  No completed tasks.\")\n        else:\n            for task in self.completed_tasks:\n                print(f\"  {task['task']} completed by {task['agent']} with status: {task['status']}\")\n        print(\"--- End of Dashboard Report ---\")\n\n    def report_overall_progress(self):\n        total_tasks = len(self.task_progress)\n        completed_tasks = len([task for task in self.task_progress if self.task_progress[task]['status'] == 'completed'])\n        success_tasks = len([task for task in self.task_completion_data if self.task_completion_data[task]['outcome'] == 'success'])\n        print(\"\\n--- Overall Task Completion Summary ---\")\n        print(f\"Total Tasks: {total_tasks}\")\n        print(f\"Completed Tasks: {completed_tasks}\")\n        print(f\"Successful Tasks: {success_tasks}\")\n        print(f\"Completion Rate: {(completed_tasks / total_tasks) * 100:.2f}%\")\n        print(f\"Success Rate: {(success_tasks / completed_tasks) * 100:.2f}% (for completed tasks)\")\n        print(\"\\n--- End of Summary ---\")\n\n    def retry_task(self, task_name, agent_name):\n        retries = self.task_retries.get(task_name, 0)\n        if retries < self.retry_limit:\n            print(f\"Retrying task '{task_name}' (Attempt {retries + 1}/{self.retry_limit})...\")\n            self.task_retries[task_name] = retries + 1\n            self.assign_tasks()\n        else:\n            print(f\"Task '{task_name}' failed after {self.retry_limit} retries.\")\n\n    def record_failure(self, task_name, agent):\n        self.task_retries[task_name] = self.task_retries.get(task_name, 0)\n        if self.task_retries[task_name] < self.retry_limit:\n            print(f\"Task '{task_name}' failed. Retrying...\")\n            self.retry_task(task_name, agent.name)\n        else:\n            print(f\"Task '{task_name}' exceeded retry limit. Marked as failed.\")\n"
        },
        ".gitignore": {
            "contents": ".pytest_cache/\n.venv/\nreal_project\n.env/\n__pycache__/"
        },
        "knowledge_base.py": {
            "contents": "class SharedKnowledgeBase:\n    def __init__(self):\n        self.data = {}\n        self.task_metadata = {}\n\n    def store(self, key, value):\n        \"\"\"\n        Stores a key-value pair in the knowledge base.\n        \"\"\"\n        self.data[key] = value\n\n    def get(self, key, default=None):\n        \"\"\"\n        Retrieves a value from the knowledge base by key.\n        Returns the default value if the key is not found.\n        \"\"\"\n        return self.data.get(key, default)\n\n    def delete(self, key):\n        \"\"\"\n        Deletes a key-value pair from the knowledge base by key.\n        \"\"\"\n        if key in self.data:\n            del self.data[key]\n\n    def list_contents(self):\n        \"\"\"\n        Lists all content currently stored in the knowledge base.\n        \"\"\"\n        if not self.data:\n            print(\"Shared knowledge base is empty.\")\n        else:\n            print(\"Shared Knowledge Base Contents:\")\n            for key, value in self.data.items():\n                print(f\"  - {key}: {value}\")\n        return self.data\n\n    def store_task_metadata(self, task_name, metadata):\n        \"\"\"\n        Stores metadata for a specific task. Metadata can include details such as task difficulty,\n        execution time, and success rate.\n        \"\"\"\n        if task_name not in self.task_metadata:\n            self.task_metadata[task_name] = []\n        self.task_metadata[task_name].append(metadata)\n        print(f\"Stored metadata for task '{task_name}': {metadata}\")\n\n    def get_task_metadata(self, task_name):\n        \"\"\"\n        Retrieves metadata for a specific task. Returns a list of metadata entries.\n        \"\"\"\n        return self.task_metadata.get(task_name, [])\n\n    def list_all_task_metadata(self):\n        \"\"\"\n        Lists all metadata stored for tasks.\n        \"\"\"\n        if not self.task_metadata:\n            print(\"No task metadata stored.\")\n        else:\n            print(\"Task Metadata Contents:\")\n            for task_name, metadata_list in self.task_metadata.items():\n                print(f\"Task '{task_name}':\")\n                for metadata in metadata_list:\n                    print(f\"  - {metadata}\")\n        return self.task_metadata\n"
        },
        "test_load_balancer.py": {
            "contents": "# test_load_balancer.py\nimport unittest\nfrom load_balancer import LoadBalancer\nfrom agents import BaseAgent\nclass TestLoadBalancer(unittest.TestCase):\n    def setUp(self):\n        self.mock_agents = [BaseAgent(\"Agent1\", {}), BaseAgent(\"Agent2\", {})]\n        self.load_balancer = LoadBalancer(self.mock_agents)\n    def test_distribute_task_evenly(self):\n        tasks = [\"task1\", \"task2\", \"task3\", \"task4\"]\n        assignments = self.load_balancer.distribute_tasks(tasks)\n        self.assertEqual(len(assignments), len(tasks))\n    def test_assign_task_to_least_loaded_agent(self):\n        task = \"new_task\"\n        agent = self.load_balancer.assign_task(task)\n        self.assertIn(agent, self.mock_agents)\n    def test_update_agent_load(self):\n        self.load_balancer.update_agent_load(\"Agent1\", 5)\n        self.assertEqual(self.load_balancer.agent_load[\"Agent1\"], 5)\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        "requirements.txt": {
            "contents": "psutil\n"
        },
        "agent_health_monitor.py": {
            "contents": "class AgentHealthMonitor:\n    \"\"\"\n    Monitors the health and performance of agents.\n    \"\"\"\n    def __init__(self, agents, load_balancer):\n        self.agent_health = {agent.name: {\"tasks_handled\": 0, \"successes\": 0, \"failures\": 0} for agent in agents}\n        self.load_balancer = load_balancer  # Adding load balancer reference to rebalance tasks\n    def record_task(self, agent_name, outcome):\n        \"\"\"\n        Records the outcome of a task handled by an agent.\n        Expects 'outcome' to be either 'success' or 'failure' and updates the agent's health accordingly.\n        \"\"\"\n        self.agent_health[agent_name][\"tasks_handled\"] += 1\n        if outcome == \"success\":\n            self.agent_health[agent_name][\"successes\"] += 1\n        elif outcome == \"failure\":\n            self.agent_health[agent_name][\"failures\"] += 1\n        else:\n            raise ValueError(f\"Unknown task outcome: {outcome}\")\n        self.display_health(agent_name)    # Displaying health after every record.\n        self.monitor_agent_health(agent_name)   # Triggering health monitoring after each task\n    def display_health(self, agent_name):\n        \"\"\"\n        Displays the current health of an agent.\n        \"\"\"\n        health = self.agent_health[agent_name]\n        print(f\"{agent_name} Health: Tasks Handled: {health['tasks_handled']}, Successes: {health['successes']}, Failures: {health['failures']}\")\n    def monitor_agent_health(self, agent_name):\n        \"\"\"\n        Checks if the agent is failing too often and triggers rebalancing if necessary.\n        \"\"\"\n        health = self.agent_health[agent_name]\n        if health['failures'] > health['successes']:\n            print(f\"Warning: {agent_name} is experiencing frequent failures.\")\n            self.trigger_rebalance(agent_name)   # Automatically rebalance if failure rate exceeds success rate.\n    def trigger_rebalance(self, failing_agent_name):\n        \"\"\"\n        Automatically reassigns tasks if an agent is failing too often.\n        \"\"\"\n        print(f\"Reassigning tasks from {failing_agent_name} due to frequent failures.\")\n        # Rebalance logic: Transfer some tasks away from the failing agent to the least busy agent.\n        rebalanced_agent = self.load_balancer.assign_task(\"Rebalance Task\")  \n        print(f\"Tasks reassigned from {failing_agent_name} to {rebalanced_agent.name}.\")\nclass LoadBalancer:\n    \"\"\"\n    Distributes tasks evenly across available agents to avoid bottlenecks.\n    \"\"\"\n    def __init__(self, agents):\n        self.agents = agents\n        self.agent_loads = {agent.name: 0 for agent in agents}\n    def assign_task(self, task):\n        \"\"\"\n        Assigns the task to the least busy agent.\n        \"\"\"\n        least_busy_agent = min(self.agent_loads, key=self.agent_loads.get)\n        print(f\"Assigning task '{task}' to {least_busy_agent}.\")\n        return [agent for agent in self.agents if agent.name == least_busy_agent][0]\n    def task_completed(self, agent_name):\n        \"\"\"\n        Marks a task as completed by the given agent.\n        \"\"\"\n        self.agent_loads[agent_name] -= 1\nclass HealthCheckManager:\n    def __init__(self, system_monitor, agent_health_monitor):\n        self.system_monitor = system_monitor\n        self.agent_health_monitor = agent_health_monitor\n    def perform_health_check(self, agent_name):\n        cpu_usage, memory_usage = self.system_monitor.monitor_resources()\n        self.agent_health_monitor.monitor_agent_health(agent_name)\n        if cpu_usage > 85 or memory_usage > 85:\n            print(f\"System overload detected. Rebalancing tasks.\")\n            self.agent_health_monitor.trigger_rebalance(agent_name)\n"
        },
        "agents.py": {
            "contents": "import os\nimport subprocess\nimport sqlite3\nimport ast\nimport inspect\nimport openai\nfrom openai import OpenAI, OpenAIError\nfrom system import TeamLeaderAI\nimport re\nfrom collections import defaultdict\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Access environment variables\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize OpenAI API client\nclient = OpenAI(api_key)\n\nclass BaseAgent:\n    def __init__(self, name, knowledge_base):\n        self.name = name\n        self.knowledge_base = knowledge_base\n        self.memory = {}\n        self.success_rate = 0.0  # Track the success rate for self-learning\n          # Replace with your actual OpenAI API key\n\n    def can_handle(self, task_name):\n        raise NotImplementedError(\"Subclasses should implement this method.\")\n\n    def execute_task(self, task_name):\n        raise NotImplementedError(\"Subclasses should implement this method.\")\n\n    def learn(self, task, outcome):\n        \"\"\"\n        Records the task outcome and adjusts the agent's success rate.\n        \"\"\"\n        if task not in self.memory:\n            self.memory[task] = {\"successes\": 0, \"failures\": 0}\n\n        # Update memory based on task outcome\n        if outcome == \"success\":\n            self.memory[task][\"successes\"] += 1\n        else:\n            self.memory[task][\"failures\"] += 1\n\n        # Calculate success rate\n        total_attempts = self.memory[task][\"successes\"] + self.memory[task][\"failures\"]\n        self.success_rate = self.memory[task][\"successes\"] / total_attempts\n\n        # Log the learning outcome\n        print(f\"{self.name} has learned from task '{task}'. Success Rate: {self.success_rate:.2%}\")\n\n        # Store the metadata in the knowledge base\n        metadata = {\n            \"task\": task,\n            \"outcome\": outcome,\n            \"success_rate\": self.success_rate,\n            \"total_attempts\": total_attempts\n        }\n        self.knowledge_base.store_task_metadata(task, metadata)\n\n    def query_improvements(self, task_name):\n        \"\"\"\n        Queries an AI model for suggestions on how to improve the task handling process.\n        \"\"\"\n        prompt = f\"{self.name} just completed a task: {task_name}. How can I improve my approach for this type of task?\"\n\n        try:\n            response = client.chat.completions.create(model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an AI assistant helping to improve task handling processes.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ])\n            improvement_suggestions = response.choices[0].message.content.strip()\n            print(f\"Improvement suggestions for {task_name}: {improvement_suggestions}\")\n            return improvement_suggestions\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except Exception as e:\n            print(f\"Unexpected Error: {e}\")\n\n    def adjust_behavior(self, task):\n        \"\"\"\n        Adjusts the agent's behavior based on its success rate for the given task.\n        \"\"\"\n        # If the success rate is low, try to adjust behavior\n        if task in self.memory and self.memory[task][\"failures\"] > self.memory[task][\"successes\"]:\n            print(f\"{self.name} adjusting behavior for task '{task}' due to low success rate.\")\n            # Example adjustment: change task strategy (e.g., increase resource allocation)\n            self.change_strategy(task)\n\n    def change_strategy(self, task):\n        \"\"\"\n        Implement a strategy change, such as increasing resource allocation or modifying the task approach.\n        This function can be customized per agent's requirements.\n        \"\"\"\n        print(f\"{self.name} is changing strategy for task '{task}' to improve performance.\")\n\nclass EnhancedAgent(BaseAgent):\n    def learn(self, task, outcome):\n        # Enhanced learning method with AI suggestions\n        super().learn(task, outcome)\n\n        if outcome == \"failure\":\n            self.adjust_strategy(task)\n\n    def adjust_strategy(self, task):\n        print(f\"{self.name} is adjusting strategy for task '{task}' due to low success rate.\")\n        # Fetch suggestions from an AI model for improvement\n        feedback = self.query_improvements(task)\n        if feedback:\n            # Implement suggestions or log them for human review\n            print(f\"Feedback for {task}: {feedback}\")\n\n\nclass ProjectArchitectAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Project Architect AI\", knowledge_base)\n\n    def can_handle(self, task):\n        return task == \"architecture design\"\n\n    def execute_task(self, task):\n        self.adjust_behavior(task)\n        print(f\"{self.name} is dynamically creating a highly sophisticated project architecture...\")\n\n        # Generate a project structure dynamically based on AI analysis\n        project_structure = self.generate_project_structure()\n        base_path = \"./dynamic_project\"\n        os.makedirs(base_path, exist_ok=True)\n\n        # Create the project files and directories\n        self.create_structure(base_path, project_structure)\n        print(f\"Dynamic project structure created by {self.name}.\")\n\n        # Learning phase\n        outcome = \"success\"\n        self.learn(task, outcome)\n        return outcome\n\n    def generate_project_structure(self):\n        \"\"\"\n        Dynamically generates a project structure based on AI-driven analysis\n        of the project type and requirements.\n        \"\"\"\n        # Use OpenAI to analyze and generate the project structure\n        prompt = (\n            f\"Generate a modular and scalable file structure for a complex {TeamLeaderAI.project_type} project. \"\n            f\"Include directories for services, handlers, utilities, tests, and documentation, with emphasis on \"\n            f\"advanced architecture patterns suitable for large-scale applications.\"\n        )\n\n        try:\n            response = client.chat.completions.create(\n                model=\"gpt-4\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are an expert software architect.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ]\n            )\n\n            structure_description = response.choices[0].message[\"content\"]\n            # Parse AI response to generate a dictionary representing the structure\n            project_structure = self.parse_structure(structure_description)\n            return project_structure\n\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except Exception as e:\n            print(f\"Error generating project structure: {e}\")\n            return self.fallback_structure()\n\n    def parse_structure(self, structure_description):\n        \"\"\"\n        Parses a text description of a file structure into a nested dictionary.\n        This implementation assumes that the AI output contains indented or\n        listed directories and files with potential comments or placeholders for content.\n        \"\"\"\n        structure = defaultdict(dict)\n        current_path = [structure]\n\n        lines = structure_description.split('\\n')\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect level of indentation to determine hierarchy\n            indent_level = len(re.match(r\"^(\\s*)\", line).group(1)) // 2  # Assuming 2 spaces per level\n\n            # Update current path to match the indent level\n            current_path = current_path[:indent_level + 1]\n            parent = current_path[-1]\n\n            # Match directory structure and files with or without content\n            dir_match = re.match(r\"^(.*):$\", line)  # Directories end with a colon \":\"\n            file_match = re.match(r\"^(.+?)\\s*-\\s*(.*)$\", line)  # Files in the form \"filename - content\"\n            empty_file_match = re.match(r\"^(.+?)$\", line)  # Files with no content described\n\n            if dir_match:\n                # Add a new directory to the current parent\n                dir_name = dir_match.group(1).strip()\n                parent[dir_name] = {}\n                current_path.append(parent[dir_name])\n            elif file_match:\n                # Add a file with specific content to the current parent\n                file_name, content = file_match.groups()\n                parent[file_name.strip()] = content.strip()\n            elif empty_file_match:\n                # Add an empty file or file with implied content\n                file_name = empty_file_match.group(1).strip()\n                parent[file_name] = \"# Content placeholder\"  # Default content if none provided\n\n        return dict(structure)\n\n    def fallback_structure(self):\n        \"\"\"\n        Provides a fallback structure if the AI generation fails.\n        \"\"\"\n        return {\n            \"src\": {\"__init__.py\": \"\"},\n            \"docs\": {\"README.md\": \"# Documentation\"},\n            \"config\": {\"config.yaml\": \"# Configurations\"},\n        }\n\n    def create_structure(self, base_path, project_structure):\n        \"\"\"\n        Recursively creates directories and files based on the provided structure with validation and error handling.\n\n        Args:\n            base_path (str): The root directory where the project structure will be created.\n            project_structure (dict): Nested dictionary representing the directories and files to create.\n        \"\"\"\n        for folder, contents in project_structure.items():\n            folder_path = os.path.join(base_path, folder)\n\n            # Attempt to create the directory and handle potential errors\n            try:\n                os.makedirs(folder_path, exist_ok=True)\n                print(f\"Directory created or already exists: {folder_path}\")\n            except OSError as e:\n                print(f\"Failed to create directory '{folder_path}': {e}\")\n                continue  # Skip this directory and continue with the next one\n\n            for file_name, file_content in contents.items():\n                if isinstance(file_content, dict):\n                    # Recursive call to handle subdirectories\n                    self.create_structure(folder_path, {file_name: file_content})\n                else:\n                    file_path = os.path.join(folder_path, file_name)\n                    try:\n                        # Attempt to create the file and write content\n                        with open(file_path, 'w') as f:\n                            f.write(file_content)\n                        print(f\"File created: {file_path}\")\n                    except IOError as e:\n                        print(f\"Failed to create file '{file_path}': {e}\")\n                        # Optionally log this error or handle it as necessary\n\nclass CodeGeneratorAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Code Generator AI\", knowledge_base)\n\n    def can_handle(self, task):\n        return task == \"code generation\"\n\n    def execute_task(self, task, project_details=None, project_type=None):\n        \"\"\"\n        Executes the code generation task with a dynamic, advanced prompt based on project details.\n\n        Args:\n            task (str): The task to be performed.\n            project_details (dict, optional): Specific details for the project, such as the type of app, features, or required technologies.\n        \"\"\"\n        # Ensure project_details has default values if not provided\n        if project_details is None:\n            project_details = {\n                \"type\": \"web app\",\n                \"architecture\": \"microservices\",\n                \"features\": [\"authentication\", \"data processing\", \"API handling\"],\n                \"technologies\": [\"Flask\", \"Redis\", \"Docker\"]\n            }\n\n        self.adjust_behavior(task)\n        print(f\"{self.name} is generating an advanced code for a {project_type}...\")\n\n        try:\n            # Generate advanced code based on detailed project requirements\n            code_content = self.generate_advanced_code(project_details)\n\n            # Define the path for the generated code\n            base_path = \"./real_project/src/\"\n            os.makedirs(base_path, exist_ok=True)\n            file_path = os.path.join(base_path, \"main.py\")\n\n            # Write the AI-generated code to a file\n            with open(file_path, 'w') as f:\n                f.write(code_content)\n\n            print(f\"Extremely advanced code generation completed for task: {task}\")\n\n            # Query for improvement suggestions after completing the task\n            improvement_suggestions = self.query_improvements(task)\n\n            # Optionally store the suggestions in the knowledge base\n            if improvement_suggestions:\n                self.knowledge_base.store(f\"{task}_improvements\", improvement_suggestions)\n\n            self.learn(task, \"success\")\n            return \"success\"\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except Exception as e:\n            print(f\"Error during code generation: {e}\")\n            self.learn(task, \"failure\")\n            return \"failure\"\n\n    def generate_advanced_code(self, project_details):\n        \"\"\"\n        Generates highly advanced, AI-driven code for a real-world application.\n\n        Args:\n            project_details (dict): Specific requirements or features for the project.\n\n        Returns:\n            str: Generated code content.\n        \"\"\"\n        # Dynamic prompt generation for an extremely advanced implementation\n        project_type = project_details.get(\"type\", \"distributed web application\")\n        architecture_style = project_details.get(\"architecture\", \"microservices with event-driven communication\")\n        main_features = project_details.get(\"features\", [\"authentication\", \"real-time data streaming\", \"state management\"])\n        technologies = project_details.get(\"technologies\", [\"Flask\", \"Redis\", \"GraphQL\", \"Kafka\", \"Docker\"])\n\n        prompt = f\"\"\"\nDesign and implement a sophisticated {project_type} with an {architecture_style} architecture.\nThe application should include:\n1. {main_features[0]} using JWT and OAuth for secure user authentication.\n2. {main_features[1]} leveraging Kafka for data streaming and Redis for caching.\n3. {main_features[2]} managed via Redux or a similar state management tool for complex UI interactions.\n4. Use advanced programming patterns such as Dependency Injection, Factory Pattern, and Repository Pattern.\n5. Implement with {', '.join(technologies)}, and ensure the application is containerized with Docker.\n6. Code should follow modular design principles, support scalability, and include error handling, logging, and monitoring.\n7. Include comprehensive comments, structured documentation, and necessary tests for all modules.\n        \"\"\"\n        try:\n            response = client.chat.completions.create(\n                model=\"gpt-4\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a code-generating AI.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ]\n            )\n            # Correct way to access the response content\n            advanced_code = response.choices[0].message.content.strip()\n            return advanced_code\n\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except Exception as e:\n            print(f\"Error querying OpenAI for advanced code generation: {e}\")\n            # Provide fallback code if API fails\n            return '''\n# Fallback advanced API setup with Microservices and Kafka for data streaming\n\nimport logging\nimport os\nfrom flask import Flask, jsonify, request\nfrom kafka import KafkaProducer\nimport redis\nimport jwt\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'your_secret_key'\nproducer = KafkaProducer(bootstrap_servers='localhost:9092')\ncache = redis.StrictRedis(host='localhost', port=6379, db=0)\n\n# Dependency Injection example\nclass ServiceInjector:\n    def __init__(self, service):\n        self._service = service\n\n    def perform_action(self):\n        self._service.execute()\n\n@app.route('/data', methods=['POST'])\ndef send_data():\n    data = request.json\n    producer.send('data-topic', bytes(str(data), 'utf-8'))\n    logging.info(\"Data sent to Kafka\")\n    return jsonify({\"status\": \"Data sent successfully\"})\n\n@app.route('/cache', methods=['GET'])\ndef get_cache():\n    value = cache.get('key')\n    return jsonify({\"cached_value\": value})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n            '''\n\nclass TestAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Test AI\", knowledge_base)\n\n    def can_handle(self, task_name):\n        return task_name == \"testing\"\n\n    def execute_task(self, task_name):\n        if task_name == \"testing\":\n            return self.run_tests()\n\n    def run_tests(self):\n        \"\"\"\n        Attempt to run tests. Logs detailed test output and retries up to 3 times if tests fail.\n        \"\"\"\n        test_dir = os.path.join(self.knowledge_base.get(\"project_path\", \"./real_project/src\"), \"tests\")\n        if not os.path.exists(test_dir):\n            print(f\"No tests directory found at {test_dir}.\")\n            return \"failure\"\n\n        try:\n            result = subprocess.run([\"pytest\", test_dir], capture_output=True, text=True)\n            print(result.stdout)\n\n            if result.returncode == 0:\n                print(\"All tests passed successfully.\")\n\n                # Query for improvement suggestions after successful testing\n                improvement_suggestions = self.query_improvements(\"testing\")\n\n                # Optionally store the suggestions in the knowledge base\n                if improvement_suggestions:\n                    self.knowledge_base.store(\"testing_improvements\", improvement_suggestions)\n\n                return \"success\"\n            else:\n                print(\"Some tests failed.\")\n                return \"failure\"\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except Exception as e:\n            print(f\"Error while running tests: {str(e)}\")\n            return \"failure\"\n\nclass DebuggingAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Debugging AI\", knowledge_base)\n\n    def can_handle(self, task):\n        return task == \"debugging\"\n\n    def execute_task(self, task):\n        self.adjust_behavior(task)  # Apply any behavior adjustments before debugging\n        print(f\"{self.name} is performing debugging on the project...\")\n\n        try:\n            # Debugging logic goes here\n            # Example debugging process\n            print(f\"Debugging task performed by {self.name}.\")\n\n            # After debugging, query for improvement suggestions\n            improvement_suggestions = self.query_improvements(task)\n\n            # Optionally store the suggestions in the knowledge base\n            if improvement_suggestions:\n                self.knowledge_base.store(f\"{task}_improvements\", improvement_suggestions)\n\n            self.learn(task, \"success\")\n            return \"success\"\n\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except Exception as e:\n            print(f\"Error during debugging: {e}\")\n            self.learn(task, \"failure\")\n            return \"failure\"\n\nclass EnhancerAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Enhancer AI\", knowledge_base)\n\n    def can_handle(self, task):\n        return task == \"enhancement\"\n\n    def execute_task(self, task):\n        self.adjust_behavior(task)  # Adjust behavior based on prior success/failure rates\n        print(f\"{self.name} is enhancing the project...\")\n\n        try:\n            # Enhancement logic goes here\n            enhancement_code = '''\ndef advanced_feature():\n    print(\"Advanced feature implemented.\")\n'''\n            base_path = \"./real_project/src/utils.py\"\n            with open(base_path, 'a') as f:\n                f.write(enhancement_code)\n\n            print(f\"Enhancement added to utils.py by {self.name}.\")\n\n            # Query for improvement suggestions after enhancement\n            improvement_suggestions = self.query_improvements(task)\n\n            # Optionally store the suggestions in the knowledge base\n            if improvement_suggestions:\n                self.knowledge_base.store(f\"{task}_improvements\", improvement_suggestions)\n\n            self.learn(task, \"success\")\n            return \"success\"\n\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except Exception as e:\n            print(f\"Error during enhancement: {e}\")\n            self.learn(task, \"failure\")\n            return \"failure\"\n\nclass DocumentationAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Documentation AI\", knowledge_base)\n\n    def can_handle(self, task_name):\n        return task_name == \"documentation\"\n\n    def execute_task(self, task_name):\n        if task_name == \"documentation\":\n            return self.generate_detailed_documentation()\n\n    def generate_detailed_documentation(self):\n        \"\"\"\n        Generates detailed documentation including:\n        1. API Documentation (function signatures)\n        2. Code Annotations (detailed explanations of code blocks)\n        3. Workflow Diagrams (general flow of the project components)\n        \"\"\"\n        print(f\"{self.name} is generating detailed documentation...\")\n\n        # Step 1: Generate API Documentation\n        self.generate_api_docs()\n\n        # Step 2: Generate Code Annotations\n        self.generate_code_annotations()\n\n        # Step 3: Generate Workflow Diagrams (Simplified as textual representation for now)\n        self.generate_workflow_diagrams()\n\n        # Query for improvement suggestions after generating documentation\n        improvement_suggestions = self.query_improvements(\"documentation\")\n\n        # Optionally store the suggestions in the knowledge base\n        if improvement_suggestions:\n            self.knowledge_base.store(\"documentation_improvements\", improvement_suggestions)\n\n        self.learn(\"documentation\", \"success\")\n        return \"success\"\n\n    def generate_api_docs(self):\n        \"\"\"\n        Generate API documentation for all Python files by extracting function definitions and signatures.\n        \"\"\"\n        print(\"Generating API documentation...\")\n        for module_name, module_ref in self.knowledge_base.get(\"modules\", {}).items():\n            print(f\"\\nModule: {module_name}\")\n            functions = inspect.getmembers(module_ref, inspect.isfunction)\n            for function_name, function_ref in functions:\n                signature = inspect.signature(function_ref)\n                print(f\"  Function: {function_name}{signature}\")\n\n    def generate_code_annotations(self):\n        \"\"\"\n        Generate code annotations by analyzing the AST (Abstract Syntax Tree) and adding comments where appropriate.\n        \"\"\"\n        print(\"Generating code annotations using AST...\")\n        for module_name, module_ref in self.knowledge_base.get(\"modules\", {}).items():\n            source_code = inspect.getsource(module_ref)\n            root = ast.parse(source_code)\n            print(f\"\\nAnnotations for {module_name}:\")\n            for node in ast.walk(root):\n                if isinstance(node, ast.FunctionDef):\n                    print(f\"  Function {node.name} is defined at line {node.lineno}.\")\n                elif isinstance(node, ast.ClassDef):\n                    print(f\"  Class {node.name} found at line {node.lineno}.\")\n\n    def generate_workflow_diagrams(self):\n        \"\"\"\n        Generate a simplified diagram of workflow/processes in the system.\n        \"\"\"\n        print(\"Generating workflow diagram...\\n\")\n        workflow = \"\"\"\n        [Team Leader AI] --> Assign Tasks\n        [Load Balancer] --> Distribute Tasks to Agents\n        [Agents] --> Perform Tasks (e.g., Code Generation, Testing, Debugging)\n        [Documentation AI] --> Generate Reports on Project State\n        \"\"\"\n        print(workflow)\n\nclass DeploymentAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Deployment AI\", knowledge_base)\n\n    def can_handle(self, task):\n        return task == \"deployment\"\n\n    def execute_task(self, task):\n        self.adjust_behavior(task)  # Adjust behavior based on previous task outcomes\n        print(f\"{self.name} is deploying the project...\")\n\n        try:\n            # Deployment logic goes here\n            dockerfile_content = '''\nFROM python:3.9-slim\nWORKDIR /app\nCOPY . /app\nRUN pip install -r requirements.txt\nCMD [\"python\", \"main.py\"]\n'''\n            with open(\"./real_project/Dockerfile\", 'w') as f:\n                f.write(dockerfile_content)\n\n            print(f\"Dockerfile created by {self.name}.\")\n\n            # Query for improvement suggestions after deployment\n            improvement_suggestions = self.query_improvements(task)\n\n            # Optionally store the suggestions in the knowledge base\n            if improvement_suggestions:\n                self.knowledge_base.store(f\"{task}_improvements\", improvement_suggestions)\n\n            self.learn(task, \"success\")\n            return \"success\"\n\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except Exception as e:\n            print(f\"Error during deployment: {e}\")\n            self.learn(task, \"failure\")\n            return \"failure\"\n\nclass SecurityAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Security AI\", knowledge_base)\n\n    def can_handle(self, task_name):\n        return task_name == \"security audit\"\n\n    def execute_task(self, task_name):\n        if task_name == \"security audit\":\n            return self.perform_security_audit()\n\n    def perform_security_audit(self):\n        \"\"\"\n        Perform a security audit, detect vulnerabilities, and attempt to fix them.\n        \"\"\"\n        print(\"Security AI is performing a security audit...\")\n\n        # Example vulnerabilities\n        vulnerabilities = [\"Insecure default configuration\", \"Weak encryption algorithm\"]\n        print(\"Vulnerabilities detected:\\n\" + \"\\n\".join(vulnerabilities))\n\n        try:\n            # Fix detected vulnerabilities\n            self.fix_vulnerabilities(vulnerabilities)\n\n            # Query for improvement suggestions after the security audit\n            improvement_suggestions = self.query_improvements(\"security audit\")\n\n            # Optionally store the suggestions in the knowledge base\n            if improvement_suggestions:\n                self.knowledge_base.store(\"security_audit_improvements\", improvement_suggestions)\n\n            return \"success\"\n\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except Exception as e:\n            print(f\"Failed to fix vulnerabilities: {str(e)}\")\n            return \"failure\"\n\n    def fix_vulnerabilities(self, vulnerabilities):\n        \"\"\"\n        Fixes known vulnerabilities. For example, updates configurations and replaces weak algorithms.\n        \"\"\"\n        for vulnerability in vulnerabilities:\n            if \"Insecure default configuration\" in vulnerability:\n                config_file = os.path.join(self.knowledge_base.get(\"project_path\", \"./real_project\"), \"config.yml\")\n                if os.path.exists(config_file):\n                    with open(config_file, 'a') as f:\n                        f.write(\"secure: true\\n\")\n                    print(\"Insecure default configuration fixed.\")\n            elif \"Weak encryption algorithm\" in vulnerability:\n                code_file = os.path.join(self.knowledge_base.get(\"project_path\", \"./real_project/src\"), \"encryption.py\")\n                if os.path.exists(code_file):\n                    with open(code_file, 'r') as f:\n                        content = f.read()\n                    updated_content = content.replace(\"AES256\", \"AES512\")\n                    with open(code_file, 'w') as f:\n                        f.write(updated_content)\n                    print(\"Weak encryption algorithm fixed.\")\n\nclass DatabaseAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Database AI\", knowledge_base)\n\n    def can_handle(self, task):\n        return task == \"database setup\"\n\n    def execute_task(self, task):\n        self.adjust_behavior(task)  # Adjust behavior based on previous task outcomes\n        print(f\"{self.name} is setting up the database...\")\n\n        try:\n            # Database setup logic goes here\n            db_path = \"./real_project/db/project.db\"\n            os.makedirs(os.path.dirname(db_path), exist_ok=True)\n\n            conn = sqlite3.connect(db_path)\n            cursor = conn.cursor()\n            cursor.execute('''\n            CREATE TABLE IF NOT EXISTS users (\n                id INTEGER PRIMARY KEY,\n                username TEXT NOT NULL,\n                email TEXT NOT NULL UNIQUE\n            )\n            ''')\n            conn.commit()\n            conn.close()\n\n            print(f\"Database created by {self.name}.\")\n\n            # Query for improvement suggestions after database setup\n            improvement_suggestions = self.query_improvements(task)\n\n            # Optionally store the suggestions in the knowledge base\n            if improvement_suggestions:\n                self.knowledge_base.store(f\"{task}_improvements\", improvement_suggestions)\n\n            self.learn(task, \"success\")\n            return \"success\"\n\n        except sqlite3.Error as e:\n            print(f\"Database setup failed: {e}\")\n            self.learn(task, \"failure\")\n            return \"failure\"\n\nclass LoggingAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Logging AI\", knowledge_base)\n\n    def can_handle(self, task):\n        return task == \"logging setup\"\n\n    def execute_task(self, task):\n        self.adjust_behavior(task)  # Adjust behavior based on past task outcomes\n        print(f\"{self.name} is setting up logging for the project...\")\n\n        try:\n            # Logging setup logic goes here\n            logging_config = '''\nimport logging\nlogging.basicConfig(filename='./real_project/logs/app.log', level=logging.INFO,\n                    format='%(asctime)s %(levelname)s: %(message)s')\nlogging.info(\"Logging is set up.\")\n'''\n            log_file_path = \"./real_project/src/logging_setup.py\"\n            os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n            with open(log_file_path, 'w') as f:\n                f.write(logging_config)\n\n            print(f\"Logging setup complete by {self.name}.\")\n\n            # Query for improvement suggestions after setting up logging\n            improvement_suggestions = self.query_improvements(task)\n\n            # Optionally store the suggestions in the knowledge base\n            if improvement_suggestions:\n                self.knowledge_base.store(f\"{task}_improvements\", improvement_suggestions)\n\n            self.learn(task, \"success\")\n            return \"success\"\n\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except Exception as e:\n            print(f\"Error during logging setup: {e}\")\n            self.learn(task, \"failure\")\n            return \"failure\"\n\nclass VersionControlAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Version Control AI\", knowledge_base)\n\n    def can_handle(self, task):\n        return task == \"version control setup\"\n\n    def execute_task(self, task):\n        self.adjust_behavior(task)  # Adjust behavior based on past task outcomes\n        print(f\"{self.name} is setting up version control for the project...\")\n\n        try:\n            # Initialize a new Git repository\n            repo_path = self.knowledge_base.get(\"project_path\", \"./real_project\")\n            os.makedirs(repo_path, exist_ok=True)\n            subprocess.run([\"git\", \"init\", repo_path], check=True)\n\n            # Create a .gitignore file\n            gitignore_content = '''\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n*.log\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# Pyre type checker\n.pyre/\n'''\n            with open(os.path.join(repo_path, \".gitignore\"), 'w') as f:\n                f.write(gitignore_content)\n\n            print(f\"Git repository initialized and .gitignore file created by {self.name}.\")\n\n            # Query for improvement suggestions after setting up version control\n            improvement_suggestions = self.query_improvements(task)\n\n            # Optionally store the suggestions in the knowledge base\n            if improvement_suggestions:\n                self.knowledge_base.store(f\"{task}_improvements\", improvement_suggestions)\n\n            self.learn(task, \"success\")\n            return \"success\"\n\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to initialize Git repository: {e}\")\n            self.learn(task, \"failure\")\n            return \"failure\"\n\nclass FrontendGeneratorAI(EnhancedAgent):\n    def __init__(self, knowledge_base):\n        super().__init__(\"Frontend Generator AI\", knowledge_base)\n\n    def can_handle(self, task):\n        return task == \"frontend generation\"\n\n    def execute_task(self, task):\n        self.adjust_behavior(task)  # Adjust behavior based on past task outcomes\n        print(f\"{self.name} is generating the frontend for the project...\")\n\n        try:\n            # Frontend generation logic goes here\n            project_path = self.knowledge_base.get(\"project_path\", \"./real_project/frontend\")\n            os.makedirs(project_path, exist_ok=True)\n\n            # Example HTML and CSS files\n            index_html = '''\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Project Frontend</title>\n    <link rel=\"stylesheet\" href=\"styles.css\">\n</head>\n<body>\n    <h1>Welcome to the Project Frontend</h1>\n    <p>This is a sample frontend generated by Frontend Generator AI.</p>\n</body>\n</html>\n'''\n            styles_css = '''\nbody {\n    font-family: Arial, sans-serif;\n    background-color: #f4f4f4;\n    color: #333;\n    text-align: center;\n    margin: 0;\n    padding: 0;\n}\nh1 {\n    color: #555;\n}\n'''\n\n            # Write files to the project directory\n            with open(os.path.join(project_path, \"index.html\"), 'w') as f:\n                f.write(index_html)\n            with open(os.path.join(project_path, \"styles.css\"), 'w') as f:\n                f.write(styles_css)\n\n            print(f\"Frontend generated successfully by {self.name}.\")\n\n            # Query for improvement suggestions after generating the frontend\n            improvement_suggestions = self.query_improvements(task)\n\n            # Optionally store the suggestions in the knowledge base\n            if improvement_suggestions:\n                self.knowledge_base.store(f\"{task}_improvements\", improvement_suggestions)\n\n            self.learn(task, \"success\")\n            return \"success\"\n\n        except OpenAIError as e:\n            print(f\"OpenAI API Error: {e}\")\n        except ValueError as ve:\n            print(f\"Value Error: {ve}\")\n        except Exception as e:\n            print(f\"Error during frontend generation: {e}\")\n            self.learn(task, \"failure\")\n            return \"failure\"\n"
        },
        "stories.json": {
            "contents": "[\n    {\n        \"id\": 1,\n        \"title\": \"Agent Enhancement and Learning\",\n        \"description\": \"I want agents to adjust their task execution behavior based on success rates, so that the system continuously improves and learns from previous task outcomes.\"\n    },\n    {\n        \"id\": 2,\n        \"title\": \"Project Setup\",\n        \"description\": \"I want to specify the project type (e.g., web app, API, etc.) so that the system can dynamically create a suitable architecture.\"\n    },\n    {\n        \"id\": 3,\n        \"title\": \"Task Prioritization\",\n        \"description\": \"I want a priority queue system that allows tasks to be dynamically reordered based on dependencies and urgency, so that high-priority tasks are handled first.\"\n    },\n    {\n        \"id\": 4,\n        \"title\": \"System Health and Monitoring\",\n        \"description\": \"I want the system to track and display the health of agents, including the number of tasks handled and success rates, so that I can monitor agent performance.\"\n    },\n    {\n        \"id\": 5,\n        \"title\": \"Dynamic Thread Management\",\n        \"description\": \"I want the system to manage dynamic thread pools based on workload, so that tasks are efficiently processed across multiple agents.\"\n    },\n    {\n        \"id\": 6,\n        \"title\": \"Testing Automation\",\n        \"description\": \"I want the system to automatically generate and execute tests for the project based on its architecture, so that I can ensure the quality of the codebase.\"\n    },\n    {\n        \"id\": 7,\n        \"title\": \"Failure Handling and Retry Logic\",\n        \"description\": \"I want the system to implement exponential backoff and retry logic for tasks that fail, so that transient issues can be automatically resolved without manual intervention.\"\n    },\n    {\n        \"id\": 8,\n        \"title\": \"Security Audit\",\n        \"description\": \"I want the system to perform security audits to detect and mitigate common vulnerabilities, so that I can ensure my project is secure.\"\n    },\n    {\n        \"id\": 9,\n        \"title\": \"Task Metadata Storage\",\n        \"description\": \"I want to store task metadata, including execution time, success rates, and task complexity, so that I can track task performance metrics.\"\n    },\n    {\n        \"id\": 10,\n        \"title\": \"Agent Collaboration and Feedback Loop\",\n        \"description\": \"I want agents to collaborate by providing feedback loops between each other when performing complex tasks, so that multi-agent tasks are coordinated efficiently.\"\n    },\n    {\n        \"id\": 11,\n        \"title\": \"Documentation Generation\",\n        \"description\": \"I want the system to generate detailed documentation, including API docs and workflow diagrams, so that I can maintain clear and up-to-date project documentation.\"\n    },\n    {\n        \"id\": 12,\n        \"title\": \"User Feedback Integration\",\n        \"description\": \"I want to provide feedback to improve system-generated features, such as project structures or code, so that the system becomes more personalized to my preferences.\"\n    },\n    {\n        \"id\": 13,\n        \"title\": \"Advanced Deployment Automation\",\n        \"description\": \"I want automated deployment scripts that include Docker configurations, so that I can deploy my project seamlessly to various environments.\"\n    },\n    {\n        \"id\": 14,\n        \"title\": \"Version Control Integration\",\n        \"description\": \"I want the system to automatically initialize and manage version control with Git, so that I can keep track of project changes without manual setup.\"\n    },\n    {\n        \"id\": 15,\n        \"title\": \"Frontend and Backend Integration\",\n        \"description\": \"I want the system to automatically generate the frontend structure, including sample HTML and CSS files, so that I can quickly get started with user interface development.\"\n    },\n    {\n        \"id\": 16,\n        \"title\": \"Error Handling and Logging\",\n        \"description\": \"I want comprehensive logging and error-handling mechanisms to be in place, so that I can troubleshoot issues effectively during development.\"\n    }\n]"
        },
        "test_health_check_manager.py": {
            "contents": "# test_health_check_manager.py\nimport unittest\nfrom health_check_manager import HealthCheckManager\nfrom agents import BaseAgent\nclass TestHealthCheckManager(unittest.TestCase):\n    def setUp(self):\n        self.mock_agents = [BaseAgent(\"Agent1\", {}), BaseAgent(\"Agent2\", {})]\n        self.health_check_manager = HealthCheckManager(self.mock_agents)\n    def test_health_check(self):\n        results = self.health_check_manager.check_health()\n        self.assertIsInstance(results, dict)\n        self.assertIn(\"Agent1\", results)\n        self.assertIn(\"Agent2\", results)\n    def test_health_status(self):\n        for agent in self.mock_agents:\n            status = self.health_check_manager.get_health_status(agent.name)\n            self.assertIn(status, [\"healthy\", \"unhealthy\"])\n    def test_recover_unhealthy_agents(self):\n        # Assuming there is logic to mark an agent as unhealthy\n        unhealthy_agents = self.health_check_manager.recover_unhealthy_agents()\n        self.assertIsInstance(unhealthy_agents, list)\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        "Dockerfile": {
            "contents": "FROM python:3.9-slim\nWORKDIR /usr/src/app\nCOPY . .\nRUN pip install --no-cache-dir -r requirements.txt\nCMD [\"python\", \"main.py\"]\n"
        },
        "test_knowledge_base.py": {
            "contents": "# test_knowledge_base.py\nimport unittest\nfrom knowledge_base import SharedKnowledgeBase\nclass TestSharedKnowledgeBase(unittest.TestCase):\n    def setUp(self):\n        self.knowledge_base = SharedKnowledgeBase()\n    def test_store_and_retrieve_knowledge(self):\n        self.knowledge_base.store(\"test_key\", \"test_value\")\n        retrieved_value = self.knowledge_base.retrieve(\"test_key\")\n        self.assertEqual(retrieved_value, \"test_value\")\n    def test_retrieve_non_existent_key(self):\n        result = self.knowledge_base.retrieve(\"non_existent_key\")\n        self.assertIsNone(result)\n    def test_remove_knowledge(self):\n        self.knowledge_base.store(\"temp_key\", \"temp_value\")\n        self.knowledge_base.remove(\"temp_key\")\n        result = self.knowledge_base.retrieve(\"temp_key\")\n        self.assertIsNone(result)\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        "test_main.py": {
            "contents": "# test_main.py\nimport unittest\nfrom unittest.mock import patch\nimport main\nclass TestMain(unittest.TestCase):\n    @patch('main.TeamLeaderAI')\n    def test_main_flow_create_project(self, MockTeamLeaderAI):\n        mock_team_leader = MockTeamLeaderAI.return_value\n        mock_team_leader.create_project.return_value = \"Project created successfully\"\n        with patch('builtins.input', side_effect=[\"1\", \"web app\"]):\n            result = main.run_program()\n            self.assertEqual(result, \"Project created successfully\")\n            mock_team_leader.create_project.assert_called_once()\n    @patch('main.TeamLeaderAI')\n    def test_main_flow_debug_project(self, MockTeamLeaderAI):\n        mock_team_leader = MockTeamLeaderAI.return_value\n        mock_team_leader.debug_project.return_value = \"Debugging completed\"\n        with patch('builtins.input', side_effect=[\"3\"]):\n            result = main.run_program()\n            self.assertEqual(result, \"Debugging completed\")\n            mock_team_leader.debug_project.assert_called_once()\n    # Add additional tests for other options (Enhance, Add Features, Test)\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        ".env": {
            "contents": "OPENAI_API_KEY = \"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\""
        },
        "task_priority_queue.py": {
            "contents": "import heapq\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nclass TaskPriorityQueue:\n    def __init__(self, system_monitor):\n        self.queue = []\n        self.system_monitor = system_monitor  # Allow access to system resource data\n        self.tasks = []  # This will hold our priority queue (min-heap)\n        self.task_dependencies = {}  # Track task dependencies\n        self.task_data = []  # Initialize task_data for AI training\n        self.model = LinearRegression()  # Model for task priority prediction\n\n    def add_task(self, priority, task_name, priority_level, dependencies=[]):\n        \"\"\"\n        Adds a task to the priority queue with AI-adjusted priority and optional dependencies.\n        \n        Args:\n            priority (int): The initial priority of the task (lower number = higher priority).\n            task_name (str): The name of the task to add.\n            priority_level (str): Task priority level, like 'high', 'medium', or 'low'.\n            dependencies (list): List of task names that must be completed before this task starts.\n        \"\"\"\n        # Check if the model is ready to make predictions (i.e., has been trained with enough data)\n        if len(self.task_data) > 50:\n            # Predict an adjusted priority based on AI model\n            adjusted_priority = self.model.predict([[priority]])[0]\n            print(f\"AI adjusted priority for '{task_name}' from {priority} to {adjusted_priority}.\")\n        else:\n            # Fallback to the original priority if not enough data\n            adjusted_priority = priority\n\n        # Add the task with the adjusted priority to the queue\n        heapq.heappush(self.tasks, (adjusted_priority, task_name, priority_level))\n        \n        # Log dependencies\n        if dependencies:\n            self.task_dependencies[task_name] = set(dependencies)\n\n        print(f\"Task '{task_name}' added with priority {adjusted_priority} and dependencies: {dependencies}\")\n        \n        # Log task data to improve future predictions\n        self.log_task_data(priority, task_name, priority_level)\n\n\n    def get_next_task(self):\n        \"\"\"\n        Retrieves the next task if its dependencies are met, handling circular dependencies.\n\n        Returns:\n            str or None: Task name if available, otherwise None.\n        \"\"\"\n        visited = set()\n\n        while self.tasks:\n            _, task_name, _ = self.tasks[0]\n\n            if self._is_circular_dependency(task_name, visited):\n                print(f\"Circular dependency detected for task '{task_name}'. Skipping task.\")\n                heapq.heappop(self.tasks)  # Remove the task from queue and continue\n                continue\n\n            # Check if dependencies are resolved\n            if all(dep not in self.task_dependencies for dep in self.task_dependencies.get(task_name, set())):\n                heapq.heappop(self.tasks)\n\n                # Only attempt to delete if task_name exists in task_dependencies\n                if task_name in self.task_dependencies:\n                    del self.task_dependencies[task_name]\n                return task_name\n            else:\n                # Dependencies not resolved, continue to the next task\n                heapq.heappop(self.tasks)\n\n        print(\"No tasks available or all have unresolved dependencies.\")\n        return None\n\n    def _is_circular_dependency(self, task_name, visited):\n        \"\"\"\n        Detects circular dependencies using a depth-first search.\n\n        Args:\n            task_name (str): The task to check for circular dependencies.\n            visited (set): Set of tasks visited in the current path.\n\n        Returns:\n            bool: True if a circular dependency is detected, otherwise False.\n        \"\"\"\n        if task_name in visited:\n            return True\n\n        visited.add(task_name)\n        for dep in self.task_dependencies.get(task_name, []):\n            if self._is_circular_dependency(dep, visited):\n                return True\n        visited.remove(task_name)\n        return False\n    \n    def display_pending_tasks(self):\n        \"\"\"\n        Display tasks currently pending in the queue, sorted by priority.\n        \"\"\"\n        if not self.queue:\n            print(\"No pending tasks.\")\n            return\n        print(\"Pending tasks:\")\n        for priority, task_name, resource_intensity in sorted(self.queue):\n            print(f\"Priority {priority}: Task '{task_name}' (Resource Intensity: {resource_intensity})\")\n\n    def log_task_data(self, priority, task_name, priority_level):\n        \"\"\"\n        Logs task data for training the AI model.\n        \n        Args:\n            priority (int): The initial priority of the task.\n            task_name (str): The name of the task to add.\n            priority_level (str): Task priority level, like 'high', 'medium', or 'low'.\n        \"\"\"\n        task_features = [priority]  # Feature vector can be expanded as needed\n        self.task_data.append((task_features, priority_level))\n\n        # Train the model with new data if enough samples are collected\n        if len(self.task_data) > 50:\n            X, y = zip(*self.task_data)\n            self.model.fit(X, y)\n            print(\"AI model for task priority has been updated with new data.\")\n            \n    def update_task_priority(self, task_name, new_priority):\n        \"\"\"\n        Updates the priority of a specific task.\n        \"\"\"\n        # Find and remove the task to update its priority\n        for i, (priority, name, level) in enumerate(self.tasks):\n            if name == task_name:\n                self.tasks.pop(i)\n                heapq.heapify(self.tasks)  # Rebalance the heap\n                self.add_task(new_priority, task_name, level)\n                print(f\"Updated priority for task '{task_name}' to {new_priority}.\")\n                return\n        print(f\"Task '{task_name}' not found in the queue.\")\n\n    def mark_task_complete(self, task_name):\n        \"\"\"\n        Removes a completed task from the dependencies of other tasks.\n        \"\"\"\n        for task, dependencies in self.task_dependencies.items():\n            if task_name in dependencies:\n                dependencies.remove(task_name)\n                print(f\"Task '{task_name}' dependency removed from '{task}'.\")\n        # Remove the task if it's no longer a dependency\n        if task_name in self.task_dependencies:\n            del self.task_dependencies[task_name]\n        print(f\"Marked task '{task_name}' as complete and resolved dependencies.\")\n"
        },
        "load_balancer.py": {
            "contents": "from sklearn.tree import DecisionTreeRegressor\nimport numpy as np\nimport psutil\nfrom knowledge_base import SharedKnowledgeBase\nimport knowledge_base\n\nclass LoadBalancer:\n    def __init__(self, agents, knowledge_base):\n        self.agents = agents\n        self.knowledge_base = knowledge_base\n        self.agent_loads = {agent.name: 0 for agent in agents}\n        self.agent_expertises = {\n            \"architecture design\": \"Project Architect AI\",\n            \"code generation\": \"Code Generator AI\",\n            \"debugging\": \"Debugging AI\",\n            \"testing\": \"Test AI\",\n            \"enhancement\": \"Enhancer AI\",\n            \"documentation\": \"Documentation AI\",\n            \"deployment\": \"Deployment AI\",\n            \"security audit\": \"Security AI\",\n            \"database setup\": \"Database AI\",\n            \"logging setup\": \"Logging AI\",\n            \"version control\": \"Version Control AI\",\n            \"frontend generation\": \"Frontend Generator AI\"\n        }\n\n    def assign_task(self, task_name):\n        \"\"\"\n        Assigns a task to the least busy or most specialized agent.\n        \n        Args:\n            task_name (str): The name of the task to assign.\n        \n        Returns:\n            The selected agent for the task.\n        \"\"\"\n        expert_agent_name = self.agent_expertises.get(task_name)\n        if expert_agent_name:\n            least_busy_agent = min(self.agent_loads, key=self.agent_loads.get)\n            assigned_agent = next((agent for agent in self.agents if agent.name == expert_agent_name), None)\n            if assigned_agent:\n                print(f\"Assigned {task_name} to specialized agent: {assigned_agent.name}\")\n            else:\n                assigned_agent = next(agent for agent in self.agents if agent.name == least_busy_agent)\n                print(f\"Assigned {task_name} to least busy agent: {assigned_agent.name}\")\n        else:\n            assigned_agent = next(agent for agent in self.agents if agent.name == min(self.agent_loads, key=self.agent_loads.get))\n            print(f\"Assigned {task_name} to least busy agent: {assigned_agent.name}\")\n        \n        self.agent_loads[assigned_agent.name] += 1\n        return assigned_agent\n\n    def estimate_task_duration(self, task_name):\n        \"\"\"\n        Estimates the duration of a task based on past task completion times.\n        \n        Args:\n            task_name (str): The name of the task to estimate.\n        \n        Returns:\n            float: The estimated duration in seconds, or None if there is no data.\n        \"\"\"\n        task_metadata = self.knowledge_base.get_task_metadata(task_name)\n        \n        if task_metadata:\n            total_time = sum(entry['duration'] for entry in task_metadata)\n            average_duration = total_time / len(task_metadata)\n            print(f\"Estimated duration for task '{task_name}': {average_duration:.2f} seconds.\")\n            return average_duration\n        else:\n            print(f\"No historical data available to estimate duration for task '{task_name}'.\")\n            return None\n\n    def task_completed(self, agent_name):\n        \"\"\"\n        Marks a task as completed for an agent, reducing their current load.\n        \n        Args:\n            agent_name (str): The name of the agent who completed a task.\n        \"\"\"\n        if agent_name in self.agent_loads:\n            self.agent_loads[agent_name] = max(0, self.agent_loads[agent_name] - 1)\n            print(f\"Marked task completed for agent: {agent_name}\")\n\n# Example agents list to simulate load balancer functioning:\n\"\"\"\nagents = {\n    \"Project Architect AI\": ProjectArchitectAI(knowledge_base),\n    \"Code Generator AI\": CodeGeneratorAI(knowledge_base),\n    \"Debugging AI\": DebuggingAI(knowledge_base),\n    \"Test AI\": TestAI(knowledge_base),\n    \"Enhancer AI\": EnhancerAI(knowledge_base),\n    \"Documentation AI\": DocumentationAI(knowledge_base),\n    \"Deployment AI\": DeploymentAI(knowledge_base),\n    \"Security AI\": SecurityAI(knowledge_base),\n    \"Database AI\": DatabaseAI(knowledge_base),\n    \"Logging AI\": LoggingAI(knowledge_base),\n    \"Version Control AI\": VersionControlAI(knowledge_base),\n    \"Frontend Generator AI\": FrontendGeneratorAI(knowledge_base)\n}\n\"\"\"\n\nclass AILoadBalancer(LoadBalancer):\n    def __init__(self, agents, knowledge_base):\n        super().__init__(agents)\n        self.data_log = []  # Collect data for training\n        self.model = DecisionTreeRegressor()  # Initialize a simple predictive model        \n        knowledge_base_instance = SharedKnowledgeBase()\n\n    def monitor_resource_usage(self):\n        # Use psutil to get system resource usage\n        cpu_usage = psutil.cpu_percent(interval=1)\n        memory_usage = psutil.virtual_memory().percent\n        disk_usage = psutil.disk_usage('/').percent\n        net_io = psutil.net_io_counters()\n        network_outgoing = net_io.bytes_sent / 1024 / 1024  # Convert to MB\n        return cpu_usage, memory_usage, disk_usage, network_outgoing\n    \n    def collect_data(self, task_name, agent_name, cpu_usage, memory_usage, task_duration, success):\n        # Log data points for training\n        self.data_log.append([cpu_usage, memory_usage, task_duration, int(success)])\n        # Keep log size manageable\n        if len(self.data_log) > 1000:\n            self.data_log.pop(0)\n\n    def train_model(self):\n        # Train model with collected data if there are enough samples\n        if len(self.data_log) >= 50:\n            X = np.array(self.data_log)[:, :3]  # cpu, memory, task duration\n            y = np.array(self.data_log)[:, 3]  # success rate\n            self.model.fit(X, y)\n\n    def estimate_task_duration(self, task_name):\n        \"\"\"\n        Estimates the duration of a task based on past task completion times.\n\n        Args:\n            task_name (str): The name of the task to estimate. This parameter is a string representing the name of the task.\n\n        Returns:\n            float: The estimated duration in seconds, or None if there is no data. The function returns a float value representing the estimated duration of the task in seconds. If no historical data is available, it returns None.\n        \"\"\"\n        # Retrieve task metadata from the knowledge base\n        task_metadata = self.knowledge_base.get_task_metadata(task_name)\n\n        # Calculate average duration if metadata is available\n        if task_metadata:\n            total_time = sum(entry['duration'] for entry in task_metadata)\n            average_duration = total_time / len(task_metadata)\n            print(f\"Estimated duration for task '{task_name}': {average_duration:.2f} seconds.\")\n            return average_duration\n        else:\n            print(f\"No historical data available to estimate duration for task '{task_name}'.\")\n            return None\n\n    def assign_task(self, task):\n        # Monitor system and predict optimal assignment\n        cpu_usage, memory_usage, _, _ = self.monitor_resource_usage()\n        task_duration = self.estimate_task_duration(task)  # Placeholder function\n        self.train_model()\n        \n        prediction = self.model.predict([[cpu_usage, memory_usage, task_duration]])\n        optimal_agent_name = self.find_optimal_agent(prediction)  # Placeholder for finding agent based on model\n        \n        # Use prediction to assign task\n        if optimal_agent_name:\n            assigned_agent = next(agent for agent in self.agents if agent.name == optimal_agent_name)\n            print(f\"AI LoadBalancer assigned task '{task}' to agent '{optimal_agent_name}'\")\n            return assigned_agent\n        else:\n            return super().assign_task(task)  # Fallback to standard method if no optimal agent is found\n\n"
        },
        "health_check_manager.py": {
            "contents": "class HealthCheckManager:\n    def __init__(self, system_monitor, agent_health_monitor):\n        self.system_monitor = system_monitor\n        self.agent_health_monitor = agent_health_monitor\n    def perform_health_check(self, agent_name):\n        cpu_usage, memory_usage = self.system_monitor.monitor_resources()\n        self.agent_health_monitor.monitor_agent_health(agent_name)\n        if cpu_usage > 85 or memory_usage > 85:\n            print(f\"System overload detected. Rebalancing tasks.\")\n"
        },
        "test_agent_health_monitor.py": {
            "contents": "# test_agent_health_monitor.py\nimport unittest\nfrom agent_health_monitor import AgentHealthMonitor, LoadBalancer\nfrom agents import BaseAgent\nclass TestAgentHealthMonitor(unittest.TestCase):\n    def setUp(self):\n        self.mock_agents = [BaseAgent(\"TestAgent\", {})]\n        self.load_balancer = LoadBalancer(self.mock_agents)\n        self.agent_health_monitor = AgentHealthMonitor(self.mock_agents, self.load_balancer)\n    def test_record_task_success(self):\n        self.agent_health_monitor.record_task(\"TestAgent\", \"success\")\n        health = self.agent_health_monitor.agent_health[\"TestAgent\"]\n        self.assertEqual(health[\"successes\"], 1)\n    def test_record_task_failure(self):\n        self.agent_health_monitor.record_task(\"TestAgent\", \"failure\")\n        health = self.agent_health_monitor.agent_health[\"TestAgent\"]\n        self.assertEqual(health[\"failures\"], 1)\n    def test_trigger_rebalance_on_failure(self):\n        self.agent_health_monitor.record_task(\"TestAgent\", \"failure\")\n        self.agent_health_monitor.record_task(\"TestAgent\", \"failure\")\n        self.agent_health_monitor.record_task(\"TestAgent\", \"failure\")\n        # Test rebalance logic as needed\n        # Add mock or print statements to confirm behavior\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        "test_task_monitor.py": {
            "contents": "# test_task_monitor.py\nimport unittest\nfrom task_monitor import TaskMonitor\nfrom agents import BaseAgent\nclass TestTaskMonitor(unittest.TestCase):\n    def setUp(self):\n        self.agent = BaseAgent(\"TestAgent\", {})\n        self.task_monitor = TaskMonitor(self.agent)\n    def test_start_task(self):\n        task_name = \"sample_task\"\n        self.task_monitor.start_task(task_name)\n        self.assertEqual(self.task_monitor.current_task, task_name)\n    def test_complete_task(self):\n        task_name = \"sample_task\"\n        self.task_monitor.start_task(task_name)\n        self.task_monitor.complete_task(\"success\")\n        self.assertIsNone(self.task_monitor.current_task)\n        self.assertIn(\"success\", self.task_monitor.task_history)\n    def test_task_failure(self):\n        task_name = \"sample_task\"\n        self.task_monitor.start_task(task_name)\n        self.task_monitor.complete_task(\"failure\")\n        self.assertIn(\"failure\", self.task_monitor.task_history)\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        "test_dynamic_thread_pool.py": {
            "contents": "# test_dynamic_thread_pool.py\nimport unittest\nfrom dynamic_thread_pool import DynamicThreadPoolExecutor\nimport time\nclass TestDynamicThreadPoolExecutor(unittest.TestCase):\n    def setUp(self):\n        self.pool = DynamicThreadPoolExecutor(max_workers=3)\n    def tearDown(self):\n        self.pool.shutdown()\n    def test_submit_task(self):\n        def sample_task(x):\n            return x * 2\n        future = self.pool.submit(sample_task, 5)\n        result = future.result()\n        self.assertEqual(result, 10)\n    def test_adjust_worker_count(self):\n        initial_count = self.pool._max_workers\n        self.pool.adjust_worker_count(5)\n        self.assertEqual(self.pool._max_workers, 5)\n    def test_task_execution_with_delay(self):\n        def delayed_task():\n            time.sleep(1)\n            return \"completed\"\n        future = self.pool.submit(delayed_task)\n        result = future.result()\n        self.assertEqual(result, \"completed\")\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        "test_system.py": {
            "contents": "# test_system.py\nimport unittest\nfrom system import System\nclass TestSystem(unittest.TestCase):\n    def setUp(self):\n        self.system = System()\n    def test_get_cpu_usage(self):\n        cpu_usage = self.system.get_cpu_usage()\n        self.assertIsInstance(cpu_usage, float)\n        self.assertGreaterEqual(cpu_usage, 0.0)\n    def test_get_memory_usage(self):\n        memory_usage = self.system.get_memory_usage()\n        self.assertIsInstance(memory_usage, float)\n        self.assertGreaterEqual(memory_usage, 0.0)\n    def test_get_disk_usage(self):\n        disk_usage = self.system.get_disk_usage()\n        self.assertIsInstance(disk_usage, float)\n        self.assertGreaterEqual(disk_usage, 0.0)\n    def test_get_network_usage(self):\n        network_usage = self.system.get_network_usage()\n        self.assertIsInstance(network_usage, float)\n        self.assertGreaterEqual(network_usage, 0.0)\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        "task_monitor.py": {
            "contents": "import time\nclass TaskMonitor:\n    def __init__(self):\n        self.task_times = {}\n        self.task_history = {}  # New dictionary to track task completion times\n    def start_task(self, task_name):\n        \"\"\"\n        Starts tracking the task's start time.\n        \"\"\"\n        self.task_times[task_name] = time.time()\n        print(f\"Started task '{task_name}' at {self.task_times[task_name]}\")\n    def end_task(self, task_name):\n        \"\"\"\n        Stops tracking the task's time and records the duration for analytics.\n        \"\"\"\n        if task_name in self.task_times:\n            elapsed_time = time.time() - self.task_times[task_name]\n            print(f\"Task '{task_name}' completed in {elapsed_time:.2f} seconds.\")\n            # Store the elapsed time in task history for tracking multiple runs\n            if task_name not in self.task_history:\n                self.task_history[task_name] = []\n            self.task_history[task_name].append(elapsed_time)\n            del self.task_times[task_name]\n            return elapsed_time\n        return None\n    def get_average_time(self, task_name):\n        \"\"\"\n        Calculates the average completion time for a given task, based on historical data.\n        \"\"\"\n        if task_name in self.task_history and self.task_history[task_name]:\n            avg_time = sum(self.task_history[task_name]) / len(self.task_history[task_name])\n            print(f\"Average time for task '{task_name}': {avg_time:.2f} seconds.\")\n            return avg_time\n        else:\n            print(f\"No historical data for task '{task_name}'.\")\n            return None\n    def display_task_statistics(self):\n        \"\"\"\n        Displays a summary of task performance statistics, including average and total run times.\n        \"\"\"\n        if self.task_history:\n            print(\"\\nTask Performance Summary:\")\n            for task_name, times in self.task_history.items():\n                total_runs = len(times)\n                total_time = sum(times)\n                average_time = total_time / total_runs if total_runs > 0 else 0\n                print(f\"Task: {task_name}, Total Runs: {total_runs}, Average Time: {average_time:.2f} seconds, Total Time: {total_time:.2f} seconds.\")\n        else:\n            print(\"No tasks have been completed yet to show statistics.\")\n"
        },
        "test_agents.py": {
            "contents": "# test_agents.py\nimport unittest\nfrom agents import ProjectArchitectAI, CodeGeneratorAI, TestAI, DebuggingAI, EnhancerAI, DocumentationAI, DeploymentAI, SecurityAI, DatabaseAI, LoggingAI, VersionControlAI, FrontendGeneratorAI\nfrom knowledge_base import SharedKnowledgeBase\nclass TestAgents(unittest.TestCase):\n    def setUp(self):\n        self.knowledge_base = SharedKnowledgeBase()\n        self.agents = [\n            ProjectArchitectAI(self.knowledge_base),\n            CodeGeneratorAI(self.knowledge_base),\n            TestAI(self.knowledge_base),\n            DebuggingAI(self.knowledge_base),\n            EnhancerAI(self.knowledge_base),\n            DocumentationAI(self.knowledge_base),\n            DeploymentAI(self.knowledge_base),\n            SecurityAI(self.knowledge_base),\n            DatabaseAI(self.knowledge_base),\n            LoggingAI(self.knowledge_base),\n            VersionControlAI(self.knowledge_base),\n            FrontendGeneratorAI(self.knowledge_base)\n        ]\n    def test_agents_can_handle(self):\n        for agent in self.agents:\n            self.assertTrue(hasattr(agent, 'can_handle'))\n    def test_agents_execute_task(self):\n        for agent in self.agents:\n            with self.assertRaises(NotImplementedError):\n                agent.execute_task('any_task')\n    # Add additional tests specific to each agent type\nif __name__ == '__main__':\n    unittest.main()\n"
        },
        "system_monitor.py": {
            "contents": "import psutil\nclass SystemMonitor:\n    def __init__(self):\n        self.alert_thresholds = {\n            'cpu': 85,  # Alert if CPU usage exceeds 85%\n            'memory': 85,  # Alert if memory usage exceeds 85%\n            'disk': 80,  # Alert if disk usage exceeds 80%\n            'network_sent': 75,  # Alert if network outgoing exceeds 75% of bandwidth (mock)\n        }\n    def monitor_resources(self):\n        \"\"\"\n        Monitors the system's resource usage.\n        \"\"\"\n        cpu_usage = psutil.cpu_percent(interval=1)\n        memory_usage = psutil.virtual_memory().percent\n        disk_usage = psutil.disk_usage('/').percent\n        network_usage_sent = self.get_network_sent_usage()\n        print(f\"CPU Usage: {cpu_usage}%, Memory Usage: {memory_usage}%, Disk Usage: {disk_usage}%, Network Outgoing: {network_usage_sent}%\")\n        # Trigger alerts if any usage exceeds pre-defined thresholds\n        self.check_alerts(cpu_usage, memory_usage, disk_usage, network_usage_sent)\n        return cpu_usage, memory_usage, disk_usage, network_usage_sent\n    def check_alerts(self, cpu_usage, memory_usage, disk_usage, network_usage_sent):\n        \"\"\"\n        Checks system metrics against pre-defined threshold limits and issues alerts if exceeded.\n        \"\"\"\n        if cpu_usage > self.alert_thresholds['cpu']:\n            print(f\"ALERT: CPU Usage exceeds {self.alert_thresholds['cpu']}% (Current: {cpu_usage}%)\")\n        if memory_usage > self.alert_thresholds['memory']:\n            print(f\"ALERT: Memory Usage exceeds {self.alert_thresholds['memory']}% (Current: {memory_usage}%)\")\n        if disk_usage > self.alert_thresholds['disk']:\n            print(f\"ALERT: Disk Usage exceeds {self.alert_thresholds['disk']}% (Current: {disk_usage}%)\")\n        if network_usage_sent > self.alert_thresholds['network_sent']:\n            print(f\"ALERT: Network Outgoing exceeds {self.alert_thresholds['network_sent']}% (Current: {network_usage_sent}%)\")\n    def get_network_sent_usage(self):\n        \"\"\"\n        Simulate network usage monitoring. You could modify this part to reflect real network usage.\n        \"\"\"\n        net_io = psutil.net_io_counters()\n        sent = net_io.bytes_sent / (1024 * 1024)  # Convert the sent bytes to megabytes\n        # For now, we'll return a mock \"percentage\", just scale bytes sent arbitrarily\n        return min(100, (sent / 5) * 10)  # This is a mock calculation assuming 5MB threshold.\n    def trigger_alert(self, alert_message):\n        \"\"\"\n        Optional: Hook this up with a notification/alerting system (e.g., email, logging, etc.).\n        \"\"\"\n        # Notifies via print for now; can change as needed (e.g., hook with logging service).\n        print(f\"Triggering Alert: {alert_message}\")\n"
        },
        "dynamic_thread_pool.py": {
            "contents": "import psutil\nfrom concurrent.futures import ThreadPoolExecutor\nclass DynamicThreadPoolExecutor:\n    def __init__(self, max_workers):\n        self.max_workers = max_workers\n        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n    def adjust_workers(self, cpu_usage, memory_usage, io_usage, network_usage):\n        \"\"\"\n        Adjust worker count based on system's resource usage, including disk I/O and network bandwidth.\n        \"\"\"\n        # Reduce the number of workers if CPU, memory, disk I/O, or network usage is too high\n        if cpu_usage > 80 or memory_usage > 80 or io_usage > 80 or network_usage > 75:\n            new_worker_count = max(1, self.max_workers // 2)  # Reduce workers to alleviate load\n            self.executor = ThreadPoolExecutor(max_workers=new_worker_count)\n            print(f\"Reducing worker count to {new_worker_count} due to high resource usage: CPU({cpu_usage}%), Memory({memory_usage}%), IO({io_usage}%), Network({network_usage}%)\")\n        # Increase workers if the system is underutilized\n        elif cpu_usage < 40 and memory_usage < 40 and io_usage < 40 and network_usage < 30:\n            new_worker_count = min(self.max_workers * 2, 10)  # Increase workers to better utilize capacity\n            self.executor = ThreadPoolExecutor(max_workers=new_worker_count)\n            print(f\"Increasing worker count to {new_worker_count} due to low resource usage: CPU({cpu_usage}%), Memory({memory_usage}%), IO({io_usage}%), Network({network_usage}%)\")\n    def monitor_resource_usage(self):\n        \"\"\"\n        Fetch system resource usage: CPU, memory, disk I/O, and network IO.\n        \"\"\"\n        cpu_usage = psutil.cpu_percent(interval=1)\n        memory_usage = psutil.virtual_memory().percent\n        io_usage = psutil.disk_io_counters().write_time   # Monitoring disk write time in milliseconds\n        network_usage = psutil.net_io_counters().bytes_sent  # Monitoring network bytes sent\n        # Normalize IO and network data\n        normalized_io_usage = min(100, (io_usage / 1000) * 10)  # Simple normalization (adjust as necessary)\n        normalized_network_usage = min(100, (network_usage / (1024 * 1024)) * 10)  # Convert bytes to MB and normalize\n        return cpu_usage, memory_usage, normalized_io_usage, normalized_network_usage\n    def submit_task(self, fn, *args, **kwargs):\n        \"\"\"\n        Submit a task to the executor, dynamically adjusting the worker count based on system resources.\n        \"\"\"\n        cpu_usage, memory_usage, io_usage, network_usage = self.monitor_resource_usage()\n        self.adjust_workers(cpu_usage, memory_usage, io_usage, network_usage)\n        return self.executor.submit(fn, *args, **kwargs)\n"
        },
        "main.py": {
            "contents": "from system import TeamLeaderAI\nfrom knowledge_base import SharedKnowledgeBase\nfrom agents import (\n    ProjectArchitectAI, CodeGeneratorAI, TestAI, EnhancerAI, DocumentationAI,\n    DeploymentAI, SecurityAI, DatabaseAI, LoggingAI, VersionControlAI, \n    FrontendGeneratorAI, DebuggingAI\n)\n\n# Initialize the shared knowledge base\nknowledge_base = SharedKnowledgeBase()\n\n# Define the collaborative agents and pass the shared knowledge base\nagents = {\n    \"Project Architect AI\": ProjectArchitectAI(knowledge_base),\n    \"Code Generator AI\": CodeGeneratorAI(knowledge_base),\n    \"Test AI\": TestAI(knowledge_base),\n    \"Enhancer AI\": EnhancerAI(knowledge_base),\n    \"Documentation AI\": DocumentationAI(knowledge_base),\n    \"Deployment AI\": DeploymentAI(knowledge_base),\n    \"Security AI\": SecurityAI(knowledge_base),\n    \"Database AI\": DatabaseAI(knowledge_base),\n    \"Logging AI\": LoggingAI(knowledge_base),\n    \"Version Control AI\": VersionControlAI(knowledge_base),\n    \"Frontend Generator AI\": FrontendGeneratorAI(knowledge_base),\n    \"Debugging AI\": DebuggingAI(knowledge_base)\n}\n\n# Initialize the Team Leader AI\nteam_leader = TeamLeaderAI(agents, knowledge_base)\n\n# Ask the user what they want the team to do\nteam_leader.receive_user_input()\n\n# Assign tasks to the agents based on their availability and priorities\nteam_leader.assign_tasks()\n\n# Example usage of the knowledge base\nknowledge_base.store(\"example_key\", \"Example knowledge\")\nprint(\"Knowledge base contents:\")\nknowledge_base.list_contents()\n\n# Initialize the AI-based Load Balancer with the agent objects only\n##load_balancer = AILoadBalancer(agents.values(), knowledge_base)\n\n# Initialize the shared knowledge base instance\n#knowledge_base_instance = SharedKnowledgeBase()\n"
        },
        "__pycache__/system_monitor.cpython-312.pyc": {
            "contents": "Could not read ./__pycache__/system_monitor.cpython-312.pyc: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte"
        },
        "__pycache__/agent_health_monitor.cpython-312.pyc": {
            "contents": "Could not read ./__pycache__/agent_health_monitor.cpython-312.pyc: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte"
        },
        "__pycache__/load_balancer.cpython-312.pyc": {
            "contents": "Could not read ./__pycache__/load_balancer.cpython-312.pyc: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte"
        },
        "__pycache__/task_priority_queue.cpython-312.pyc": {
            "contents": "Could not read ./__pycache__/task_priority_queue.cpython-312.pyc: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte"
        },
        "__pycache__/agents.cpython-312.pyc": {
            "contents": "Could not read ./__pycache__/agents.cpython-312.pyc: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte"
        },
        "__pycache__/health_check_manager.cpython-312.pyc": {
            "contents": "Could not read ./__pycache__/health_check_manager.cpython-312.pyc: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte"
        },
        "__pycache__/task_monitor.cpython-312.pyc": {
            "contents": "Could not read ./__pycache__/task_monitor.cpython-312.pyc: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte"
        },
        "__pycache__/knowledge_base.cpython-312.pyc": {
            "contents": "Could not read ./__pycache__/knowledge_base.cpython-312.pyc: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte"
        },
        "__pycache__/system.cpython-312.pyc": {
            "contents": "Could not read ./__pycache__/system.cpython-312.pyc: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte"
        },
        "__pycache__/dynamic_thread_pool.cpython-312.pyc": {
            "contents": "Could not read ./__pycache__/dynamic_thread_pool.cpython-312.pyc: 'utf-8' codec can't decode byte 0xcb in position 0: invalid continuation byte"
        },
        "zDocs/call_graph.png": {
            "contents": "Could not read ./zDocs/call_graph.png: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte"
        },
        ".venv/.gitignore": {
            "contents": "*"
        },
        ".venv/pyvenv.cfg": {
            "contents": "home = /home/codespace/.python/current/bin\ninclude-system-site-packages = false\nversion = 3.12.1\nexecutable = /usr/local/python/3.12.1/bin/python3.12\ncommand = /home/codespace/.python/current/bin/python3 -m venv /workspaces/AI-Team/.venv\n"
        },
        "zScripts/unpackStructure.py": {
            "contents": "import os\n\n# Function to parse the Project_Overview.txt file and extract the structure\ndef parse_project_overview(file_path):\n    project_structure = {}\n    current_path = []\n    \n    with open(file_path, 'r') as file:\n        for line in file:\n            # Skip empty lines\n            if line.strip() == \"\":\n                continue\n            \n            # Detect file header and extract file name correctly\n            if line.startswith(\"--- File: \"):\n                # Extract and clean file name, removing extra spaces and dashes\n                file_name = line.split(\"--- File: \")[1].strip().strip(\"-\").strip()\n                file_parts = file_name.split(\"/\")\n                current_path = file_parts[:-1]\n                current_file = file_parts[-1].strip()  # Strip any trailing whitespace\n                \n                # Traverse into the path and initialize structure\n                current_dir = project_structure\n                for folder in current_path:\n                    current_dir = current_dir.setdefault(folder, {})\n                current_dir[current_file] = \"\"\n            \n            # Otherwise, it's content of the current file\n            else:\n                # Append content to the current file\n                current_dir[current_file] += line\n\n    return project_structure\n\n# Function to create directories and files based on the parsed structure\ndef create_project_structure(base_path, structure):\n    for name, content in structure.items():\n        path = os.path.join(base_path, name.strip())  # Ensure the name is stripped of whitespace\n        if isinstance(content, dict):\n            # Create directory if it's a dictionary\n            os.makedirs(path, exist_ok=True)\n            create_project_structure(path, content)\n        else:\n            # Create and write to file if it's not a dictionary\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, 'w') as f:\n                f.write(content)\n\n# Main function to process the Project_Overview.txt and create the project\ndef main(project_overview_path, base_path=\"./project\"):\n    # Parse the project structure from Project_Overview.txt\n    project_structure = parse_project_overview(project_overview_path)\n    \n    # Create directories and files according to the parsed structure\n    create_project_structure(base_path, project_structure)\n    \n    print(f\"Project structure created at '{base_path}' based on '{project_overview_path}'.\")\n\n# Example usage\nmain(\"Project_Overview.txt\", \".\")\n"
        },
        "zScripts/createOverview.py": {
            "contents": "import os\ndef combine_files_in_directory(directory, output_file):\n    with open(output_file, 'w') as outfile:\n        # Traverse the directory recursively\n        for root, dirs, files in os.walk(directory):\n            # Ignore directories that start with '__' or are hidden (dot directories)\n            dirs[:] = [d for d in dirs if not d.startswith('__') and not d.startswith('.') and not d.startswith('real')]\n            for file in files:\n                # Ignore dot files (hidden files)\n                if file.startswith('.'):\n                    continue\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, directory)\n                try:\n                    with open(file_path, 'r') as infile:\n                        # Write the relative file path as a header\n                        outfile.write(f\"\\n\\n--- File: {relative_path} ---\\n\\n\")\n                        outfile.write(infile.read())\n                        outfile.write(\"\\n\")  # Add a newline after file content\n                except Exception as e:\n                    print(f\"Could not read {file_path}: {e}\")\nif __name__ == \"__main__\":\n    directory = input(\"Enter the directory to combine files from: \")\n    output_file = \"Project_Overview.txt\"\n    combine_files_in_directory(directory, output_file)\n    print(f\"All files have been combined into {output_file}.\")\n"
        },
        "zScripts/SNAPSHOT.py": {
            "contents": "import os\nimport ast\nimport json\nimport platform\nimport subprocess\nfrom importlib.metadata import distributions  # Replacement for pkg_resources\n\n# List of files and directories to ignore\nIGNORE_LIST = [\n    '.env',\n    '__pycache__/',\n    '.pytest_cache/',\n    '.venv/',\n    '.git',\n    'zScripts/',\n    'zScripts/SNAPSHOT.py',\n    '.gitignore',\n    'requirements.txt',\n    'setup.py',\n    'package.json',\n    'node_modules/',\n    'real_project/src',\n    'zDocs/'\n]\n\ndef should_ignore(path):\n    \"\"\"Check if the given path matches any of the ignore patterns.\"\"\"\n    for ignore in IGNORE_LIST:\n        if ignore in path:\n            return True\n    return False\n\ndef get_system_info():\n    \"\"\"Capture system information like OS, Python version, and installed libraries.\"\"\"\n    installed_libraries = [f\"{dist.metadata['Name']}=={dist.version}\" for dist in distributions()]\n    return {\n        \"os\": platform.system(),\n        \"os_version\": platform.version(),\n        \"python_version\": platform.python_version(),\n        \"installed_libraries\": installed_libraries\n    }\n\ndef get_file_contents(file_path):\n    \"\"\"Return the contents of a file.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read()\n    except Exception as e:\n        return f\"Could not read {file_path}: {str(e)}\"\n\ndef get_function_calls(file_path):\n    \"\"\"Extract function calls using AST from a Python file.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            tree = ast.parse(file.read(), filename=file_path)\n    except (SyntaxError, UnicodeDecodeError) as e:\n        return {}\n\n    function_calls = {}\n\n    for node in ast.walk(tree):\n        if isinstance(node, ast.FunctionDef):\n            function_name = node.name\n            calls = []\n            for sub_node in ast.walk(node):\n                if isinstance(sub_node, ast.Call) and isinstance(sub_node.func, ast.Name):\n                    calls.append(sub_node.func.id)\n            function_calls[function_name] = {\n                \"outgoing_calls\": calls,\n                \"incoming_calls\": []\n            }\n\n    return function_calls\n\ndef update_incoming_calls(function_calls_dict):\n    \"\"\"Reverse engineer the outgoing calls to create incoming calls.\"\"\"\n    for file, functions in function_calls_dict.items():\n        for func, details in functions.items():\n            for called_func in details['outgoing_calls']:\n                for f_file, f_functions in function_calls_dict.items():\n                    if called_func in f_functions:\n                        f_functions[called_func]['incoming_calls'].append(func)\n\ndef get_dependencies():\n    \"\"\"Retrieve dependencies from common files like requirements.txt or package.json.\"\"\"\n    dependencies = {}\n    if os.path.exists(\"requirements.txt\"):\n        with open(\"requirements.txt\", 'r', encoding='utf-8') as f:\n            dependencies['python'] = f.read().splitlines()\n    if os.path.exists(\"package.json\"):\n        try:\n            result = subprocess.run(['npm', 'list', '--json'], stdout=subprocess.PIPE, text=True)\n            package_json = json.loads(result.stdout)\n            dependencies['node'] = package_json.get('dependencies', {})\n        except Exception as e:\n            dependencies['node'] = f\"Could not retrieve node dependencies: {str(e)}\"\n    return dependencies\n\ndef get_directory_structure(directory):\n    \"\"\"Walk through the directory and collect file paths.\"\"\"\n    file_structure = {}\n    for root, dirs, files in os.walk(directory):\n        if should_ignore(root):\n            continue\n        relative_root = os.path.relpath(root, directory)\n        file_structure[relative_root] = files\n    return file_structure\n\ndef get_git_history():\n    \"\"\"Get the latest Git history.\"\"\"\n    try:\n        result = subprocess.run(['git', 'log', '--pretty=format:%h - %s', '-n', '10'], stdout=subprocess.PIPE, text=True)\n        return result.stdout.splitlines()\n    except Exception as e:\n        return f\"Could not retrieve Git history: {str(e)}\"\n\ndef get_project_info(directory):\n    \"\"\"Aggregate all project information into a dictionary.\"\"\"\n    project_info = {\n        \"system_info\": get_system_info(),\n        \"file_structure\": get_directory_structure(directory),\n        \"dependencies\": get_dependencies(),\n        \"git_history\": get_git_history(),\n        \"files\": {}\n    }\n\n    function_calls_dict = {}\n\n    for root, dirs, files in os.walk(directory):\n        if should_ignore(root):\n            continue\n        for file in files:\n            file_path = os.path.join(root, file)\n            relative_path = os.path.relpath(file_path, directory)\n            if file.endswith(\".py\"):\n                function_calls = get_function_calls(file_path)\n                if function_calls:\n                    function_calls_dict[relative_path] = function_calls\n            project_info['files'][relative_path] = {\n                \"contents\": get_file_contents(file_path)\n            }\n\n    update_incoming_calls(function_calls_dict)\n    project_info[\"function_calls\"] = function_calls_dict\n\n    return project_info\n\ndef save_as_json(project_info, output_file):\n    \"\"\"Save the entire project information as a JSON file.\"\"\"\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(project_info, f, indent=4)\n    print(f\"Project information saved to {output_file}\")\n\nif __name__ == \"__main__\":\n    directory = input(\"Enter the directory path: \")\n    output_file = \"zDocs/project_info.json\"\n\n    if not os.path.isdir(directory):\n        print(\"Invalid directory path\")\n    else:\n        project_info = get_project_info(directory)\n        save_as_json(project_info, output_file)\n"
        },
        "zScripts/createCallGraph.py": {
            "contents": "import os\nimport ast\nimport json\n\n# List of files and directories to ignore\nIGNORE_LIST = [\n    'real_project/src/',\n    '__pycache__/',\n    '.pytest_cache/',\n    '.venv/',\n    'zDocs/',\n    'zScripts/',\n    '.gitignore',\n    'requirements.txt'\n]\n\ndef should_ignore(path):\n    # Check if the path starts with any of the ignore list items\n    for ignore in IGNORE_LIST:\n        if ignore in path:\n            return True\n    return False\n\ndef get_functions_and_classes_from_file(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            tree = ast.parse(file.read(), filename=file_path)\n    except (SyntaxError, UnicodeDecodeError) as e:\n        print(f\"Skipping {file_path} due to a parsing error: {e}\")\n        return {}, {}\n\n    function_calls = {}\n\n    for node in ast.walk(tree):\n        if isinstance(node, ast.FunctionDef):\n            function_name = node.name\n            calls = []\n            # Collect calls made inside the function\n            for sub_node in ast.walk(node):\n                if isinstance(sub_node, ast.Call) and isinstance(sub_node.func, ast.Name):\n                    calls.append(sub_node.func.id)\n            function_calls[function_name] = {\n                \"outgoing_calls\": calls,\n                \"incoming_calls\": []  # To be populated later\n            }\n\n    return function_calls\n\ndef update_incoming_calls(function_calls_dict):\n    # Reverse engineer the outgoing calls to create incoming calls\n    for file, functions in function_calls_dict.items():\n        for func, details in functions.items():\n            for called_func in details['outgoing_calls']:\n                # Look for the called function and add the current function as incoming\n                for f_file, f_functions in function_calls_dict.items():\n                    if called_func in f_functions:\n                        f_functions[called_func]['incoming_calls'].append(func)\n\ndef get_functions_and_classes_from_directory(directory):\n    function_calls_dict = {}\n\n    for root, dirs, files in os.walk(directory):\n        # Skip directories and files in the ignore list\n        if should_ignore(root):\n            continue\n\n        for file in files:\n            file_path = os.path.join(root, file)\n            if should_ignore(file_path) or not file.endswith(\".py\"):\n                continue\n\n            function_calls = get_functions_and_classes_from_file(file_path)\n            if function_calls:\n                function_calls_dict[file_path] = function_calls\n\n    update_incoming_calls(function_calls_dict)\n    return function_calls_dict\n\ndef save_as_json(function_calls_dict, output_file):\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(function_calls_dict, f, indent=4)\n    print(f\"Call graph data saved to {output_file}\")\n\nif __name__ == \"__main__\":\n    directory = input(\"Enter the directory path: \")\n    output_file = \"zScripts/Calls/function_calls.json\"\n\n    if not os.path.isdir(directory):\n        print(\"Invalid directory path\")\n    else:\n        function_calls_dict = get_functions_and_classes_from_directory(directory)\n\n        # Save function calls as JSON\n        save_as_json(function_calls_dict, output_file)\n"
        },
        ".pytest_cache/.gitignore": {
            "contents": "# Created by pytest automatically.\n*\n"
        },
        ".pytest_cache/CACHEDIR.TAG": {
            "contents": "Signature: 8a477f597d28d172789f06886806bc55\n# This file is a cache directory tag created by pytest.\n# For information about cache directory tags, see:\n#\thttps://bford.info/cachedir/spec.html\n"
        },
        ".pytest_cache/README.md": {
            "contents": "# pytest cache directory #\n\nThis directory contains data from the pytest's cache plugin,\nwhich provides the `--lf` and `--ff` options, as well as the `cache` fixture.\n\n**Do not** commit this to version control.\n\nSee [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.\n"
        }
    },
    "function_calls": {
        "create_github_issues.py": {
            "create_github_issue": {
                "outgoing_calls": [
                    "print",
                    "print"
                ],
                "incoming_calls": [
                    "main"
                ]
            },
            "main": {
                "outgoing_calls": [
                    "create_github_issue"
                ],
                "incoming_calls": []
            }
        },
        "test_task_priority_queue.py": {
            "setUp": {
                "outgoing_calls": [
                    "TaskPriorityQueue"
                ],
                "incoming_calls": []
            },
            "test_enqueue_task": {
                "outgoing_calls": [
                    "len"
                ],
                "incoming_calls": []
            },
            "test_dequeue_task": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_queue_emptiness": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_peek_task": {
                "outgoing_calls": [],
                "incoming_calls": []
            }
        },
        "system.py": {
            "__init__": {
                "outgoing_calls": [
                    "TaskPriorityQueue",
                    "DynamicThreadPoolExecutor",
                    "LoadBalancer",
                    "TaskMonitor",
                    "AgentHealthMonitor",
                    "SystemMonitor",
                    "HealthCheckManager",
                    "SystemMonitor"
                ],
                "incoming_calls": []
            },
            "get_user_input": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "input",
                    "int"
                ],
                "incoming_calls": []
            },
            "ask_for_project_path": {
                "outgoing_calls": [
                    "input",
                    "print"
                ],
                "incoming_calls": []
            },
            "receive_user_input": {
                "outgoing_calls": [
                    "print",
                    "input",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "decompose_project": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "assign_tasks": {
                "outgoing_calls": [
                    "set",
                    "hasattr",
                    "print"
                ],
                "incoming_calls": []
            },
            "find_agent_for_task": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "execute_task": {
                "outgoing_calls": [
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "handle_agent_feedback": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "recover_from_failure": {
                "outgoing_calls": [
                    "min",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "update_task_status": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "mark_task_completed": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "report_progress": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "report_overall_progress": {
                "outgoing_calls": [
                    "len",
                    "len",
                    "len",
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "retry_task": {
                "outgoing_calls": [
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "record_failure": {
                "outgoing_calls": [
                    "print",
                    "print"
                ],
                "incoming_calls": []
            }
        },
        "knowledge_base.py": {
            "__init__": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "store": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "get": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "delete": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "list_contents": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "store_task_metadata": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            },
            "get_task_metadata": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "list_all_task_metadata": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            }
        },
        "test_load_balancer.py": {
            "setUp": {
                "outgoing_calls": [
                    "LoadBalancer",
                    "BaseAgent",
                    "BaseAgent"
                ],
                "incoming_calls": []
            },
            "test_distribute_task_evenly": {
                "outgoing_calls": [
                    "len",
                    "len"
                ],
                "incoming_calls": []
            },
            "test_assign_task_to_least_loaded_agent": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_update_agent_load": {
                "outgoing_calls": [],
                "incoming_calls": []
            }
        },
        "agent_health_monitor.py": {
            "__init__": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "record_task": {
                "outgoing_calls": [
                    "ValueError"
                ],
                "incoming_calls": []
            },
            "display_health": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            },
            "monitor_agent_health": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            },
            "trigger_rebalance": {
                "outgoing_calls": [
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "assign_task": {
                "outgoing_calls": [
                    "min",
                    "print"
                ],
                "incoming_calls": []
            },
            "task_completed": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "perform_health_check": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            }
        },
        "agents.py": {
            "__init__": {
                "outgoing_calls": [
                    "super"
                ],
                "incoming_calls": []
            },
            "can_handle": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "execute_task": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "open",
                    "open",
                    "print",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "learn": {
                "outgoing_calls": [
                    "super"
                ],
                "incoming_calls": []
            },
            "query_improvements": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "adjust_behavior": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            },
            "change_strategy": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            },
            "adjust_strategy": {
                "outgoing_calls": [
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "generate_project_structure": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "parse_structure": {
                "outgoing_calls": [
                    "defaultdict",
                    "dict",
                    "len"
                ],
                "incoming_calls": []
            },
            "fallback_structure": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "create_structure": {
                "outgoing_calls": [
                    "print",
                    "isinstance",
                    "print",
                    "print",
                    "open",
                    "print"
                ],
                "incoming_calls": []
            },
            "generate_advanced_code": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "run_tests": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "str"
                ],
                "incoming_calls": []
            },
            "generate_detailed_documentation": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            },
            "generate_api_docs": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "generate_code_annotations": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "isinstance",
                    "print",
                    "isinstance",
                    "print"
                ],
                "incoming_calls": []
            },
            "generate_workflow_diagrams": {
                "outgoing_calls": [
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "perform_security_audit": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "print",
                    "print",
                    "print",
                    "str"
                ],
                "incoming_calls": []
            },
            "fix_vulnerabilities": {
                "outgoing_calls": [
                    "print",
                    "open",
                    "print",
                    "open",
                    "open"
                ],
                "incoming_calls": []
            }
        },
        "test_health_check_manager.py": {
            "setUp": {
                "outgoing_calls": [
                    "HealthCheckManager",
                    "BaseAgent",
                    "BaseAgent"
                ],
                "incoming_calls": []
            },
            "test_health_check": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_health_status": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_recover_unhealthy_agents": {
                "outgoing_calls": [],
                "incoming_calls": []
            }
        },
        "test_knowledge_base.py": {
            "setUp": {
                "outgoing_calls": [
                    "SharedKnowledgeBase"
                ],
                "incoming_calls": []
            },
            "test_store_and_retrieve_knowledge": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_retrieve_non_existent_key": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_remove_knowledge": {
                "outgoing_calls": [],
                "incoming_calls": []
            }
        },
        "test_main.py": {
            "test_main_flow_create_project": {
                "outgoing_calls": [
                    "patch",
                    "patch"
                ],
                "incoming_calls": []
            },
            "test_main_flow_debug_project": {
                "outgoing_calls": [
                    "patch",
                    "patch"
                ],
                "incoming_calls": []
            }
        },
        "task_priority_queue.py": {
            "__init__": {
                "outgoing_calls": [
                    "LinearRegression"
                ],
                "incoming_calls": []
            },
            "add_task": {
                "outgoing_calls": [
                    "print",
                    "len",
                    "print",
                    "set"
                ],
                "incoming_calls": []
            },
            "get_next_task": {
                "outgoing_calls": [
                    "set",
                    "print",
                    "all",
                    "print",
                    "set"
                ],
                "incoming_calls": []
            },
            "_is_circular_dependency": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "display_pending_tasks": {
                "outgoing_calls": [
                    "print",
                    "sorted",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "log_task_data": {
                "outgoing_calls": [
                    "len",
                    "zip",
                    "print"
                ],
                "incoming_calls": []
            },
            "update_task_priority": {
                "outgoing_calls": [
                    "enumerate",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "mark_task_complete": {
                "outgoing_calls": [
                    "print",
                    "print"
                ],
                "incoming_calls": []
            }
        },
        "load_balancer.py": {
            "__init__": {
                "outgoing_calls": [
                    "DecisionTreeRegressor",
                    "SharedKnowledgeBase",
                    "super"
                ],
                "incoming_calls": []
            },
            "assign_task": {
                "outgoing_calls": [
                    "next",
                    "print",
                    "super"
                ],
                "incoming_calls": []
            },
            "estimate_task_duration": {
                "outgoing_calls": [
                    "sum",
                    "print",
                    "print",
                    "len"
                ],
                "incoming_calls": []
            },
            "task_completed": {
                "outgoing_calls": [
                    "max",
                    "print"
                ],
                "incoming_calls": []
            },
            "monitor_resource_usage": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "collect_data": {
                "outgoing_calls": [
                    "len",
                    "int"
                ],
                "incoming_calls": []
            },
            "train_model": {
                "outgoing_calls": [
                    "len"
                ],
                "incoming_calls": []
            }
        },
        "health_check_manager.py": {
            "__init__": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "perform_health_check": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            }
        },
        "test_agent_health_monitor.py": {
            "setUp": {
                "outgoing_calls": [
                    "LoadBalancer",
                    "AgentHealthMonitor",
                    "BaseAgent"
                ],
                "incoming_calls": []
            },
            "test_record_task_success": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_record_task_failure": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_trigger_rebalance_on_failure": {
                "outgoing_calls": [],
                "incoming_calls": []
            }
        },
        "test_task_monitor.py": {
            "setUp": {
                "outgoing_calls": [
                    "BaseAgent",
                    "TaskMonitor"
                ],
                "incoming_calls": []
            },
            "test_start_task": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_complete_task": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_task_failure": {
                "outgoing_calls": [],
                "incoming_calls": []
            }
        },
        "test_dynamic_thread_pool.py": {
            "setUp": {
                "outgoing_calls": [
                    "DynamicThreadPoolExecutor"
                ],
                "incoming_calls": []
            },
            "tearDown": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_submit_task": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_adjust_worker_count": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_task_execution_with_delay": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "sample_task": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "delayed_task": {
                "outgoing_calls": [],
                "incoming_calls": []
            }
        },
        "test_system.py": {
            "setUp": {
                "outgoing_calls": [
                    "System"
                ],
                "incoming_calls": []
            },
            "test_get_cpu_usage": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_get_memory_usage": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_get_disk_usage": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "test_get_network_usage": {
                "outgoing_calls": [],
                "incoming_calls": []
            }
        },
        "task_monitor.py": {
            "__init__": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "start_task": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            },
            "end_task": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            },
            "get_average_time": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "sum",
                    "len"
                ],
                "incoming_calls": []
            },
            "display_task_statistics": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "len",
                    "sum",
                    "print"
                ],
                "incoming_calls": []
            }
        },
        "test_agents.py": {
            "setUp": {
                "outgoing_calls": [
                    "SharedKnowledgeBase",
                    "ProjectArchitectAI",
                    "CodeGeneratorAI",
                    "TestAI",
                    "DebuggingAI",
                    "EnhancerAI",
                    "DocumentationAI",
                    "DeploymentAI",
                    "SecurityAI",
                    "DatabaseAI",
                    "LoggingAI",
                    "VersionControlAI",
                    "FrontendGeneratorAI"
                ],
                "incoming_calls": []
            },
            "test_agents_can_handle": {
                "outgoing_calls": [
                    "hasattr"
                ],
                "incoming_calls": []
            },
            "test_agents_execute_task": {
                "outgoing_calls": [],
                "incoming_calls": []
            }
        },
        "system_monitor.py": {
            "__init__": {
                "outgoing_calls": [],
                "incoming_calls": []
            },
            "monitor_resources": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            },
            "check_alerts": {
                "outgoing_calls": [
                    "print",
                    "print",
                    "print",
                    "print"
                ],
                "incoming_calls": []
            },
            "get_network_sent_usage": {
                "outgoing_calls": [
                    "min"
                ],
                "incoming_calls": []
            },
            "trigger_alert": {
                "outgoing_calls": [
                    "print"
                ],
                "incoming_calls": []
            }
        },
        "dynamic_thread_pool.py": {
            "__init__": {
                "outgoing_calls": [
                    "ThreadPoolExecutor"
                ],
                "incoming_calls": []
            },
            "adjust_workers": {
                "outgoing_calls": [
                    "max",
                    "ThreadPoolExecutor",
                    "print",
                    "min",
                    "ThreadPoolExecutor",
                    "print"
                ],
                "incoming_calls": []
            },
            "monitor_resource_usage": {
                "outgoing_calls": [
                    "min",
                    "min"
                ],
                "incoming_calls": []
            },
            "submit_task": {
                "outgoing_calls": [],
                "incoming_calls": []
            }
        },
        "zScripts/unpackStructure.py": {
            "parse_project_overview": {
                "outgoing_calls": [
                    "open"
                ],
                "incoming_calls": [
                    "main"
                ]
            },
            "create_project_structure": {
                "outgoing_calls": [
                    "isinstance",
                    "create_project_structure",
                    "open"
                ],
                "incoming_calls": [
                    "create_project_structure",
                    "main"
                ]
            },
            "main": {
                "outgoing_calls": [
                    "parse_project_overview",
                    "create_project_structure",
                    "print"
                ],
                "incoming_calls": []
            }
        },
        "zScripts/createOverview.py": {
            "combine_files_in_directory": {
                "outgoing_calls": [
                    "open",
                    "open",
                    "print"
                ],
                "incoming_calls": []
            }
        },
        "zScripts/SNAPSHOT.py": {
            "should_ignore": {
                "outgoing_calls": [],
                "incoming_calls": [
                    "get_directory_structure",
                    "get_project_info",
                    "get_functions_and_classes_from_directory",
                    "get_functions_and_classes_from_directory"
                ]
            },
            "get_system_info": {
                "outgoing_calls": [
                    "distributions"
                ],
                "incoming_calls": [
                    "get_project_info"
                ]
            },
            "get_file_contents": {
                "outgoing_calls": [
                    "open",
                    "str"
                ],
                "incoming_calls": [
                    "get_project_info"
                ]
            },
            "get_function_calls": {
                "outgoing_calls": [
                    "isinstance",
                    "open",
                    "isinstance",
                    "isinstance"
                ],
                "incoming_calls": [
                    "get_project_info"
                ]
            },
            "update_incoming_calls": {
                "outgoing_calls": [],
                "incoming_calls": [
                    "get_project_info",
                    "get_functions_and_classes_from_directory"
                ]
            },
            "get_dependencies": {
                "outgoing_calls": [
                    "open",
                    "str"
                ],
                "incoming_calls": [
                    "get_project_info"
                ]
            },
            "get_directory_structure": {
                "outgoing_calls": [
                    "should_ignore"
                ],
                "incoming_calls": [
                    "get_project_info"
                ]
            },
            "get_git_history": {
                "outgoing_calls": [
                    "str"
                ],
                "incoming_calls": [
                    "get_project_info"
                ]
            },
            "get_project_info": {
                "outgoing_calls": [
                    "update_incoming_calls",
                    "get_system_info",
                    "get_directory_structure",
                    "get_dependencies",
                    "get_git_history",
                    "should_ignore",
                    "get_function_calls",
                    "get_file_contents"
                ],
                "incoming_calls": []
            },
            "save_as_json": {
                "outgoing_calls": [
                    "print",
                    "open"
                ],
                "incoming_calls": []
            }
        },
        "zScripts/createCallGraph.py": {
            "should_ignore": {
                "outgoing_calls": [],
                "incoming_calls": [
                    "get_directory_structure",
                    "get_project_info",
                    "get_functions_and_classes_from_directory",
                    "get_functions_and_classes_from_directory"
                ]
            },
            "get_functions_and_classes_from_file": {
                "outgoing_calls": [
                    "isinstance",
                    "open",
                    "print",
                    "isinstance",
                    "isinstance"
                ],
                "incoming_calls": [
                    "get_functions_and_classes_from_directory"
                ]
            },
            "update_incoming_calls": {
                "outgoing_calls": [],
                "incoming_calls": [
                    "get_project_info",
                    "get_functions_and_classes_from_directory"
                ]
            },
            "get_functions_and_classes_from_directory": {
                "outgoing_calls": [
                    "update_incoming_calls",
                    "should_ignore",
                    "get_functions_and_classes_from_file",
                    "should_ignore"
                ],
                "incoming_calls": []
            },
            "save_as_json": {
                "outgoing_calls": [
                    "print",
                    "open"
                ],
                "incoming_calls": []
            }
        }
    }
}